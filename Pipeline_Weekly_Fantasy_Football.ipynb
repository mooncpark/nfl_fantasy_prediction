{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QW6m2N7lV9AH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mooncpark/nfl_fantasy_prediction/blob/main/Pipeline_Weekly_Fantasy_Football.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRjHnuXn_fym",
        "outputId": "1fc0052b-a5e1-4547-917d-dcdaaf0c89e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nfl_data_py in /usr/local/lib/python3.9/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pandas>1 in /usr/local/lib/python3.9/dist-packages (from nfl_data_py) (1.5.3)\n",
            "Requirement already satisfied: appdirs>1 in /usr/local/lib/python3.9/dist-packages (from nfl_data_py) (1.4.4)\n",
            "Requirement already satisfied: fastparquet>0.5 in /usr/local/lib/python3.9/dist-packages (from nfl_data_py) (2023.2.0)\n",
            "Requirement already satisfied: python-snappy>0.5 in /usr/local/lib/python3.9/dist-packages (from nfl_data_py) (0.6.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from fastparquet>0.5->nfl_data_py) (2023.4.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from fastparquet>0.5->nfl_data_py) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fastparquet>0.5->nfl_data_py) (23.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.9/dist-packages (from fastparquet>0.5->nfl_data_py) (2.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>1->nfl_data_py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>1->nfl_data_py) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>1->nfl_data_py) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nfl_data_py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwmDx0WpVy5P",
        "outputId": "e0f7793e-14fc-4fb2-c531-822d953758a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.9/dist-packages (0.6.2.20230320)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from snscrape) (3.11.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from snscrape) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from snscrape) (2.27.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->snscrape) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8FuGE-ePhsCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJ-42FBPVv8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nfl_data_py as nfl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "#preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#twitter scraper for sentiment analysis\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "#modeling\n",
        "from sklearn.svm import SVR, LinearSVC\n",
        "from sklearn.linear_model import LinearRegression,LassoCV,Ridge,Lasso,ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "#eval\n",
        "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, RepeatedKFold, GridSearchCV\n",
        "from sklearn.feature_selection import RFE, SelectKBest, chi2, f_regression, VarianceThreshold, mutual_info_regression, RFE\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import mean, std\n",
        "\n",
        "\n",
        "#Viz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3vNb5Cbd_jVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cac7016-c74b-4cf8-d12d-a6c97eac62b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0e1yL7qR_jt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALERT, if you don't want to be sitting around here forever (~15 mins per season) waiting for a twitter scraper, please make sure the three 2020-2022 tweets CSVs included in our repository are uploaded to your local session :)"
      ],
      "metadata": {
        "id": "IcqBtrWrzIGp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QWIRg91iTNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This pipeline has been broken into chunks - function definitions and then execution blocks where the functions are called. "
      ],
      "metadata": {
        "id": "QgZVIS2O_vQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction Step 1 - Data Extraction from nfl_data_py"
      ],
      "metadata": {
        "id": "exX0cPoxOGtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract season data from nfl_data_py, both inputs taken as list (int of each season, list of columns)\n",
        "#assumption is that current season will be updated after each week of games, if not sooner\n",
        "#daily pipeline will repeatedly extract current season\n",
        "\n",
        "def extract_all_season_data_dfs(seasons, pb_columns, wk_columns):\n",
        "  print('Now extracting all dataframes from nfl_data_py')\n",
        "  pbp_data = nfl.import_pbp_data(years=seasons, downcast=False, cache=False, alt_path=None)\n",
        "  pbp_data = pbp_data[pb_columns] #limit to relevant columns, parameter for method call in nfl_data_py appears to not work\n",
        "  pbp_data = pbp_data[(pbp_data['play'] == 1.0) & (pbp_data['season_type']=='REG')] #only non-special teams plays (pass or rush) and for regular season weeks (fantasy season)\n",
        "  pbp_data.fillna(np.nan, inplace=True) #replace None types\n",
        "  pbp_data['season'] = [int(g[:4]) for g in pbp_data['game_id']]\n",
        "\n",
        "  week_data = nfl.import_weekly_data(years=seasons, columns=wk_columns, downcast=False) #limit to relevant columns\n",
        "  week_data = week_data[(week_data['position'].isin(['QB','RB','WR','TE'])&(week_data['season_type']=='REG'))] #only fantasy relevant positions & for regular season weeks (fantasy season)\n",
        "\n",
        "\n",
        "  sched_data = nfl.import_schedules(years=seasons) #contains game meta data\n",
        "\n",
        "  rosters_data = nfl.import_rosters(years=seasons) #contains team level roster info and player profile data - might be a good way to split rookies, but unclear if updated in season\n",
        "\n",
        "  injuries_data = nfl.import_injuries(years=seasons) #contains injury reports\n",
        "\n",
        "  combine_data = nfl.import_combine_data(years=seasons, positions=['QB','RB','WR','TE']) #only fantasy relevant positions\n",
        "\n",
        "  depth_data = nfl.import_depth_charts(years=seasons) #contains depth chart information\n",
        "\n",
        "  latest_season = max(seasons) # to identify latest season in data\n",
        "\n",
        "  return pbp_data, week_data, sched_data, rosters_data, injuries_data, combine_data, depth_data, latest_season"
      ],
      "metadata": {
        "id": "tRUSoWXX_kGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 1 - Derive Additional Counting Stats from Play-by-Play Data"
      ],
      "metadata": {
        "id": "t1wEfhyVOXR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to create some additional counting stats at the play level for additional potential features\n",
        "\n",
        "def additional_pbp_counting_stats(pbp_df):\n",
        "  print('Now using play-by-play data to identify specific fantasy relevant plays')\n",
        "  pbp_df['compl_passes_20_yds'] = np.where(((pbp_df['complete_pass'] == 1.0) & (pbp_df['passing_yards'] >= 20.0)), 1, 0)\n",
        "  pbp_df['compl_passes_40_yds'] = np.where(((pbp_df['complete_pass'] == 1.0) & (pbp_df['passing_yards'] >= 40.0)), 1, 0)\n",
        "  pbp_df['compl_passes_60_yds'] = np.where(((pbp_df['complete_pass'] == 1.0) & (pbp_df['passing_yards'] >= 60.0)), 1, 0)\n",
        "  pbp_df['pass_att_20_yds'] = np.where(((pbp_df['pass_attempt'] == 1.0) & (pbp_df['air_yards'] >= 20.0)), 1, 0)\n",
        "  pbp_df['pass_att_40_yds'] = np.where(((pbp_df['pass_attempt'] == 1.0) & (pbp_df['air_yards'] >= 40.0)), 1, 0)\n",
        "  pbp_df['pass_att_60_yds'] = np.where(((pbp_df['pass_attempt'] == 1.0) & (pbp_df['air_yards'] >= 60.0)), 1, 0)\n",
        "  pbp_df['pass_att_goal'] = np.where(((pbp_df['pass_attempt']==1.0) & (pbp_df['yardline_100'] <= 5)), 1, 0)\n",
        "  pbp_df['pass_att_red'] = np.where(((pbp_df['pass_attempt']==1.0) & (pbp_df['yardline_100'] <= 20)), 1, 0)\n",
        "  pbp_df['rush_20_yds'] = np.where(((pbp_df['rush_attempt']==1.0) & (pbp_df['rushing_yards'] >= 20.0)), 1, 0)\n",
        "  pbp_df['rush_40_yds'] = np.where(((pbp_df['rush_attempt']==1.0) & (pbp_df['rushing_yards'] >= 40.0)), 1, 0)\n",
        "  pbp_df['rush_60_yds'] = np.where(((pbp_df['rush_attempt']==1.0) & (pbp_df['rushing_yards'] >= 60.0)), 1, 0)\n",
        "  pbp_df['goal_rush'] = np.where(((pbp_df['rush_attempt']==1.0) & (pbp_df['yardline_100'] <= 5)), 1, 0)\n",
        "  pbp_df['red_rush'] = np.where(((pbp_df['rush_attempt']==1.0) & (pbp_df['yardline_100'] <= 20)), 1, 0)\n",
        "  return pbp_df\n"
      ],
      "metadata": {
        "id": "XTTqFg5yAHkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play By Play (PBP) Metrics not included in weekly data:\n",
        "\n",
        "Player Level:  Goal line rushes, Red Zone Rushes, Red Zone Targets, Red Zone Pass Att, Deep Pass Att, Deep Compl Pass Attempts, Deep Targets, Big Plays\n",
        "\n",
        "Team Offense Level: Total Valuable Plays, Total Rushes, Total Passes, Goal line rushes, Red Zone Rushes, Red Zone Targets, Red Zone Pass Att, Deep Pass Att, Deep Compl Pass Attempts, Deep Targets, Big Plays\n",
        "\n",
        "Team Defense Level: Yards Allowed, Pass Attempts Allowed, Completions Allowed, Rushes allowed, YPA allowed, YPC allowed, YAC allowed, Long passes allowed, Long rushes allowed"
      ],
      "metadata": {
        "id": "-4QmBs4K4sZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 2"
      ],
      "metadata": {
        "id": "vHsqTHnwOnqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to collapse all pbp data to player id level, offense team level, and defense team level all at game level -- returns multiple dfs\n",
        "\n",
        "def aggregate_pbp_data(pbp_df):\n",
        "  print('Now aggregating all play-by-play data to week level and breaking into play type categories (pass, rush, receive, total offense, total defense)')\n",
        "  player_game_df_pass = pbp_df.groupby(['game_id','season','week','passer_id','defteam']).agg({'compl_passes_20_yds':'sum','compl_passes_40_yds':'sum','compl_passes_60_yds':'sum',\n",
        "                                                                                  'pass_att_20_yds':'sum','pass_att_40_yds':'sum','pass_att_60_yds':'sum',\n",
        "                                                                                  'pass_att_goal':'sum','pass_att_red':'sum','sack':'sum'}).reset_index()\n",
        "  player_game_df_pass = player_game_df_pass.rename(columns={'game_id':'game_id_p','defteam':'defteam_pas'}) #rename to avoid dupe column names\n",
        "\n",
        "  player_game_df_rush = pbp_df.groupby(['game_id','season','week','rusher_id','defteam']).agg({'rush_20_yds':'sum','rush_40_yds':'sum','rush_60_yds':'sum',\n",
        "                                                                                  'goal_rush':'sum','red_rush':'sum'}).reset_index()\n",
        "  player_game_df_rush = player_game_df_rush.rename(columns={'game_id':'game_id_rus','defteam':'defteam_rus'}) #rename to avoid dupe column names\n",
        "\n",
        "\n",
        "  player_game_df_receive = pbp_df.groupby(['game_id','season','week','receiver_id','defteam']).agg({'compl_passes_20_yds':'sum','compl_passes_40_yds':'sum','compl_passes_60_yds':'sum',\n",
        "                                                                                    'pass_att_20_yds':'sum','pass_att_40_yds':'sum','pass_att_60_yds':'sum',\n",
        "                                                                                    'pass_att_goal':'sum','pass_att_red':'sum'}).reset_index()\n",
        "  player_game_df_receive.columns = ['game_id_rec','season','week','defteam_rec','receiver_id','catches_20_yards','catches_40_yards','catches_60_yards',\n",
        "                                    'targets_20_yards','targets_40_yards','targets_60_yards','targets_goal','targets_red'] #rename receiving stats to avoid confusion with passing stats\n",
        "\n",
        "\n",
        "  offense_game_df = pbp_df.groupby(['game_id','season','week','posteam','defteam']).agg({'compl_passes_20_yds':'sum','compl_passes_40_yds':'sum','compl_passes_60_yds':'sum',\n",
        "                                                          'pass_att_20_yds':'sum','pass_att_40_yds':'sum','pass_att_60_yds':'sum',\n",
        "                                                          'pass_att_goal':'sum','pass_att_red':'sum','goal_rush':'sum','red_rush':'sum',\n",
        "                                                           'pass_attempt':'sum','rush_attempt':'sum'}).reset_index()\n",
        "                      \n",
        "  offense_game_df.columns = ['game_id_o','season','week','posteam','defteam_o','team_20_yard_compl_o','team_40_yard_compl_o','team_60_yard_compl_o',\n",
        "                             'team_20_yard_pass_att_o','team_40_yard_pass_att_o','team_60_yard_pass_att_o','team_goal_pass_att_o',\n",
        "                             'team_red_pass_att_o','team_goal_rush_att_o','team_red_rush_att_o','team_pass_att_o','team_rush_att_o'\n",
        "                            ]             \n",
        "\n",
        "  defense_game_df = pbp_df.groupby(['game_id','season','week','defteam']).agg({'compl_passes_20_yds':'sum','compl_passes_40_yds':'sum','compl_passes_60_yds':'sum',\n",
        "                                                          'pass_att_20_yds':'sum','pass_att_40_yds':'sum','pass_att_60_yds':'sum',\n",
        "                                                          'pass_att_goal':'sum','pass_att_red':'sum','goal_rush':'sum','red_rush':'sum',\n",
        "                                                           'pass_attempt':'sum','complete_pass':'sum','rush_attempt':'sum','pass_touchdown':'sum',\n",
        "                                                           'rush_touchdown':'sum','passing_yards':'sum','rushing_yards':'sum',\n",
        "                                                           'sack':'sum','yards_after_catch':'sum'}).reset_index()\n",
        "  \n",
        "\n",
        "  defense_game_df['ypa_allowed'] = defense_game_df['passing_yards']/defense_game_df['pass_attempt'] # defense yards per attempt allowed\n",
        "  defense_game_df['ypc_allowed'] = defense_game_df['rushing_yards']/defense_game_df['rush_attempt'] # defense yards per carry allowed\n",
        "  defense_game_df['yac_per_compl_allowed'] = defense_game_df['yards_after_catch']/defense_game_df['complete_pass'] # defense yards after catch per catch allowed\n",
        "\n",
        "  defense_game_df.columns = ['game_id','season','week','defteam','team_20_yard_compl_d','team_40_yard_compl_d','team_60_yard_compl_d',\n",
        "                             'team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_60_yard_pass_att_d','team_goal_pass_att_d',\n",
        "                             'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed',\n",
        "                             'team_rush_att_d','pass_tds_allowed','rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed',\n",
        "                             'ypa_allowed','ypc_allowed','yac_per_compl_allowed'\n",
        "                            ]\n",
        "\n",
        "  return player_game_df_pass, player_game_df_rush, player_game_df_receive, offense_game_df, defense_game_df                                                      "
      ],
      "metadata": {
        "id": "vmIfQhtwFu27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 3"
      ],
      "metadata": {
        "id": "g-7utZ1kOuSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to merge new player level stats & offense + defense team level stats, to weekly stats -- return master player-level weekly df\n",
        "\n",
        "def create_master_weekly_player_data(week_data, player_game_df_pass, player_game_df_rush, player_game_df_receive, offense_game_df, defense_game_df):\n",
        "  #add 0.5 PPR (point per reception) scoring\n",
        "\n",
        "  week_data['fantasy_points_halfppr'] = week_data['fantasy_points_ppr'] - (0.5*week_data['receptions'])\n",
        "  \n",
        "  print('Now creating weekly historical positional rankings by each score format')\n",
        "\n",
        "  weekly_pos_rankings = pd.DataFrame()\n",
        "  for scoring_format in ['fantasy_points','fantasy_points_halfppr','fantasy_points_ppr']:\n",
        "    scoring_df = week_data[['player_id','season','week','position',scoring_format]].copy()\n",
        "    sort_scoring_df = scoring_df.sort_values(by=['season','week','position',scoring_format], ascending=[True,True,True,False])\n",
        "    sort_scoring_df['wk_pos_rank-{}'.format(scoring_format)] = sort_scoring_df.groupby(['season','week','position']).cumcount()+1;sort_scoring_df\n",
        "    sort_scoring_df = sort_scoring_df.set_index(keys=['player_id','season','week','position'])\n",
        "    weekly_pos_rankings = pd.concat([weekly_pos_rankings,sort_scoring_df],join='outer',axis=1, ignore_index=False)\n",
        "  weekly_pos_rankings = weekly_pos_rankings.reset_index()\n",
        "  \n",
        "  print('Now combining all player stats data at the player and week level')\n",
        "  master_wk_df = pd.merge(week_data,player_game_df_pass,how='left', left_on=['player_id','season','week'], right_on=['passer_id','season','week'])\n",
        "  master_wk_df = pd.merge(master_wk_df,player_game_df_rush,how='left', left_on=['player_id','season','week'], right_on=['rusher_id','season','week'])\n",
        "  master_wk_df = pd.merge(master_wk_df,player_game_df_receive,how='left', left_on=['player_id','season','week'], right_on=['receiver_id','season','week'])\n",
        "  master_wk_df = pd.merge(master_wk_df,offense_game_df,how='left', left_on=['season','week','recent_team'], right_on=['season','week','posteam'])\n",
        "\n",
        "  master_wk_df.drop_duplicates(inplace=True)\n",
        "\n",
        "  #add 0.5 PPR (point per reception) scoring\n",
        "\n",
        "  master_wk_df['fantasy_points_halfppr'] = master_wk_df['fantasy_points_ppr'] - (0.5*master_wk_df['receptions'])\n",
        "\n",
        "  #create rate data fields\n",
        "\n",
        "  master_wk_df = master_wk_df.fillna(np.nan)\n",
        "  master_wk_df = master_wk_df.replace(np.nan, 0)\n",
        "\n",
        "  master_wk_df['ypa'] = master_wk_df['passing_yards']/master_wk_df['attempts'] #yards per pass attempt\n",
        "  master_wk_df['air_ypa'] = master_wk_df['passing_air_yards']/master_wk_df['attempts'] #air yards per pass attempt\n",
        "  master_wk_df['completion_per'] = master_wk_df['completions']/master_wk_df['attempts'] #passing completion percent\n",
        "  master_wk_df['passing_td_rate'] = master_wk_df['passing_tds']/master_wk_df['attempts'] #passing td rate\n",
        "\n",
        "\n",
        "  master_wk_df['goal_rush_team_per'] = master_wk_df['goal_rush']/master_wk_df['team_goal_rush_att_o'] #player percent of total team goaline rushes\n",
        "  master_wk_df['red_rush_team_per'] = master_wk_df['red_rush']/master_wk_df['team_red_rush_att_o'] #player percent of total team redzone rushes\n",
        "  master_wk_df['tot_rush_team_per'] = master_wk_df['carries']/master_wk_df['team_rush_att_o'] #player percent of total team rushes \n",
        "\n",
        "  master_wk_df['goal_targ_team_per'] = master_wk_df['targets_goal']/master_wk_df['team_goal_pass_att_o'] #player percent of total team goaline targets\n",
        "  master_wk_df['red_targ_team_per'] = master_wk_df['targets_red']/master_wk_df['team_red_pass_att_o'] #player percent of total team redzone rushes\n",
        "\n",
        "  #print(master_wk_df.columns)\n",
        "  # clean up individual weekly player performance df\n",
        "\n",
        "  new_cols = ['player_id','player_display_name','position','recent_team','season','week','defteam_o',\n",
        "              'season_type','passing_yards','passing_tds','interceptions','sacks','sack_fumbles','sack_fumbles_lost','passing_air_yards',\n",
        "              'compl_passes_20_yds','compl_passes_40_yds','ypa','air_ypa','completion_per','passing_td_rate','pass_att_goal','pass_att_red',\n",
        "              'carries','rushing_yards','rushing_tds','rushing_fumbles','rushing_first_downs','rush_20_yds','rush_40_yds',\n",
        "              'goal_rush_team_per','red_rush_team_per','tot_rush_team_per',\n",
        "              'receptions','targets','receiving_yards','receiving_tds','receiving_fumbles',\n",
        "              'receiving_air_yards','receiving_yards_after_catch','receiving_first_downs','catches_20_yards','catches_40_yards',\n",
        "              'target_share','air_yards_share','goal_targ_team_per','red_targ_team_per',\n",
        "              'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr'\n",
        "              ]\n",
        "\n",
        "  trim_master_wk_df = master_wk_df[new_cols].copy()\n",
        "  #trim_master_wk_df = trim_master_wk_df.rename(columns={'defteam_o':'defteam'}) #rename to avoid dupe column names\n",
        "\n",
        "  return trim_master_wk_df, weekly_pos_rankings\n"
      ],
      "metadata": {
        "id": "RjZGujT7UwA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation Step 4"
      ],
      "metadata": {
        "id": "tm9HeYYiOxYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_schedule_df(sched_data, rosters_data, depth_data, injuries_data):\n",
        "  #weekly game level meta data\n",
        "  print('Now creating a dataframe containing schedule, team roster, depth chart and injuries at the player and week level')\n",
        "  sched_cols = ['game_id', 'season', 'game_type', 'week', 'gameday', 'weekday',\n",
        "              'gametime', 'div_game', 'roof', 'surface', 'temp', 'wind','away_team','home_team']\n",
        "\n",
        "  trim_sched_data = sched_data[sched_cols].copy()\n",
        "  trim_sched_data['teams'] = trim_sched_data[['away_team','home_team']].values.tolist()\n",
        "\n",
        "\n",
        "  trim_sched_data = trim_sched_data.replace(np.nan,0)\n",
        "  trim_sched_data = trim_sched_data[trim_sched_data['game_type'] == 'REG']\n",
        "  trim_sched_data = trim_sched_data.explode(column='teams')\n",
        "  trim_sched_data = trim_sched_data.drop(labels=['away_team','home_team'], axis=1)\n",
        "\n",
        "  #depth chart\n",
        "  depth_cols = ['gsis_id', 'season', 'week', 'game_type', 'position', 'club_code', 'depth_team']\n",
        "\n",
        "  depth_data_trim = depth_data[depth_cols]\n",
        "  depth_data_trim = depth_data_trim[depth_data_trim['game_type'] == 'REG']\n",
        "  depth_data_trim = depth_data_trim[depth_data_trim['position'].isin(['QB','WR','RB','TE'])]\n",
        "  depth_data_trim.sort_values(by=['season','week','club_code','gsis_id','depth_team'], ascending=True, inplace=True) #player duplicates in weekly depth chart\n",
        "\n",
        "  #keep highest depth player entry for each weekly player record\n",
        "  depth_data_trim.drop_duplicates(subset=['gsis_id', 'season', 'week', 'game_type', 'position', 'club_code'],keep='first',inplace=True) \n",
        "  #merge weekly game with depth chart\n",
        "\n",
        "  sched_dep = pd.merge(trim_sched_data, depth_data_trim, how='left', left_on=['season', 'week', 'teams','game_type'], right_on=['season', 'week', 'club_code','game_type'])\n",
        "\n",
        "  #player records via team roster at the end of each season, including player status\n",
        "  roster_cols = ['player_id', 'player_name', 'season', 'status',\n",
        "               'college', 'height', 'weight', 'years_exp', 'entry_year', 'rookie_year', 'draft_club',\n",
        "               'draft_number']\n",
        "\n",
        "  rosters_data_trim = rosters_data[roster_cols]\n",
        "  rosters_data_trim = rosters_data_trim.drop_duplicates()\n",
        "\n",
        "  #merge weekly game data with rosters\n",
        "\n",
        "  sched_dep_ros = pd.merge(sched_dep, rosters_data_trim, how='left', left_on=['season', 'gsis_id'], right_on=['season', 'player_id'])\n",
        "\n",
        "  #create simple season, player, week, team df\n",
        "\n",
        "  team_df = sched_dep_ros[['player_id','season','week','teams']].copy()\n",
        "\n",
        "  team_df = team_df.drop_duplicates()\n",
        "\n",
        "  #add indicator for rookies\n",
        "\n",
        "  sched_dep_ros['rookie'] = np.where(sched_dep_ros['rookie_year'] == sched_dep_ros['season'], 1,0)\n",
        "\n",
        "  #merge with injuries data\n",
        "\n",
        "  injuries_cols = ['season', 'week', 'gsis_id', 'report_status'] #report_status has None, Questionable, Doubtful, Out\n",
        "\n",
        "  injuries_data.fillna(np.nan, inplace=True)\n",
        "\n",
        "  injuries_data_fil = injuries_data[(injuries_data['game_type'] == 'REG') & (~injuries_data['report_status'].isnull())].copy() #only regular season injury reports and non\n",
        "\n",
        "  injuries_data_fil = injuries_data_fil[injuries_cols].copy()\n",
        "\n",
        "  sched_dep_ros_inj = pd.merge(sched_dep_ros, injuries_data_fil, how='left', left_on=['season', 'gsis_id','week'], right_on=['season', 'gsis_id','week'])\n",
        "\n",
        "  return sched_dep_ros_inj, team_df\n"
      ],
      "metadata": {
        "id": "hKHkeaim4BSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 5"
      ],
      "metadata": {
        "id": "0DP4BC0dO0po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need to aggregate weekly data for weeks leading up to current week\n",
        "\n",
        "def agg_player_stats_weeks_leading_up(sched_dep_ros_inj, trim_master_wk_df, pos_rankings_df, defense_game_df, current_season, current_week): #need to set current_season and current week\n",
        "  print('Now aggregating player stats up to Current Season: {} and Current Week: {}'.format(str(current_season), str(current_week)))\n",
        "  #add defteam for every game to sched df, to allow for all defense stats to be added even if player did not eventually play\n",
        "  defteam_df = defense_game_df[['game_id','defteam']]\n",
        "  defteam_df = defteam_df.drop_duplicates()\n",
        "  sched_dep_ros_inj_def = pd.merge(sched_dep_ros_inj,defteam_df,how='left',on='game_id')\n",
        "  sched_dep_ros_inj_def = sched_dep_ros_inj_def[sched_dep_ros_inj_def['defteam'] != sched_dep_ros_inj_def['teams']].copy()\n",
        "\n",
        "  #merge sched df with weekly player master df\n",
        "\n",
        "  full_player_df = pd.merge(sched_dep_ros_inj_def,weekly_mas_df,how='left',left_on=['player_id','season','week'],right_on=['player_id','season','week'])\n",
        "  full_player_df['game_played'] = np.where(full_player_df['player_display_name'].isnull(), 0, 1) #derive game played flag based on pbp data accumulated\n",
        "\n",
        "  #merge in weekly positional rankings\n",
        "  pos_rankings_trim = pos_rankings_df[['player_id','season','week','wk_pos_rank-fantasy_points',\n",
        "                                       'wk_pos_rank-fantasy_points_halfppr','wk_pos_rank-fantasy_points_ppr']].copy()\n",
        "\n",
        "  full_player_df = pd.merge(full_player_df,pos_rankings_trim,how='inner',on=['player_id','season','week'])\n",
        "\n",
        "  #filter to all player data before current season and current week\n",
        "  full_player_df_lead = full_player_df[((full_player_df['season'] == current_season) & (full_player_df['week'] < current_week)) |\n",
        "                                       (full_player_df['season'] < current_season)].copy()\n",
        "\n",
        "  #only consider games where player accumulated stats\n",
        "  full_player_active = full_player_df_lead[full_player_df_lead['game_played']==1].copy()\n",
        "\n",
        "  stat_cols = ['passing_yards', 'passing_tds', 'interceptions', 'sacks',\n",
        "       'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards',\n",
        "       'compl_passes_20_yds', 'compl_passes_40_yds', 'ypa', 'air_ypa',\n",
        "       'completion_per', 'passing_td_rate', 'pass_att_goal', 'pass_att_red',\n",
        "       'carries', 'rushing_yards', 'rushing_tds', 'rushing_fumbles',\n",
        "       'rushing_first_downs', 'rush_20_yds', 'rush_40_yds',\n",
        "       'goal_rush_team_per', 'red_rush_team_per', 'tot_rush_team_per',\n",
        "       'receptions', 'targets', 'receiving_yards', 'receiving_tds',\n",
        "       'receiving_fumbles', 'receiving_air_yards',\n",
        "       'receiving_yards_after_catch', 'receiving_first_downs',\n",
        "       'catches_20_yards', 'catches_40_yards', 'target_share',\n",
        "       'air_yards_share', 'goal_targ_team_per', 'red_targ_team_per',\n",
        "       'wk_pos_rank-fantasy_points','wk_pos_rank-fantasy_points_halfppr',\n",
        "       'wk_pos_rank-fantasy_points_ppr','fantasy_points','fantasy_points_halfppr',\n",
        "       'fantasy_points_ppr'\n",
        "       ]\n",
        "\n",
        "  #group and average stats -- possible future enhancement: should we also include 1) stdev/variances? 2) avg or stdev/variance of fantasy points as features?)\n",
        "  #also include total number of games played in each season leading up to current season/week\n",
        "  full_player_games_played = full_player_active.groupby(['player_id','season'])['game_played'].sum().reset_index()\n",
        "  full_player_active_grp = full_player_active.groupby(['player_id','season'])[stat_cols].mean().reset_index()\n",
        "\n",
        "  full_player_active_grp = pd.merge(full_player_games_played,full_player_active_grp,how='inner',on=['player_id','season'])\n",
        "\n",
        "  #calc standard deviation for the positional rank and score variables for each scoring format leading up to current week\n",
        "  std_cols = ['wk_pos_rank-fantasy_points','wk_pos_rank-fantasy_points_halfppr',\n",
        "       'wk_pos_rank-fantasy_points_ppr','fantasy_points','fantasy_points_halfppr',\n",
        "       'fantasy_points_ppr']\n",
        "\n",
        "  full_player_active_std = full_player_active.groupby(['player_id','season'])[std_cols].std().reset_index()\n",
        "\n",
        "  full_player_active_std.columns = ['player_id','season'] + [s+'-std' for s in std_cols]\n",
        "\n",
        "  #merge player leading up data with that player's data for the current week and through the rest of the season\n",
        "\n",
        "  full_player_df_current = full_player_df[(full_player_df['season'] == current_season) & (full_player_df['week'] >= current_week)]\n",
        "\n",
        "  full_player_active_grp.columns = ['player_id','season','game_played_sum'] + [s + '_avg' for s in stat_cols] #add 'avg' to each avg stat col, excl player id and season\n",
        "\n",
        "\n",
        "  full_player_df_current_and_avg = pd.merge(full_player_df_current,full_player_active_grp,\n",
        "                                          how='left',left_on=['player_id','season'],right_on=['player_id','season'])##need to remove season for weighting of previous seasons\n",
        "\n",
        "  #create df with stdevs and avgs for weeks leading up and actuals for each player, season - only for pos rankings and points\n",
        "\n",
        "  full_player_df_current_and_avg = pd.merge(full_player_df_current_and_avg,full_player_active_std,how='inner', on=['player_id','season'])\n",
        "\n",
        "  #rename _avg cols used in prep for MLP classification (Modeling Prep Step 3) to parse scoring system correctly\n",
        "\n",
        "  rename_col_dict = {'wk_pos_rank-fantasy_points_avg':'wk_pos_rank-fantasy_points-avg',\n",
        "          'wk_pos_rank-fantasy_points_halfppr_avg':'wk_pos_rank-fantasy_points_halfppr-avg',\n",
        "          'wk_pos_rank-fantasy_points_ppr_avg':'wk_pos_rank-fantasy_points_ppr-avg',\n",
        "          'fantasy_points_avg':'fantasy_points-avg','fantasy_points_halfppr_avg':'fantasy_points_halfppr-avg',\n",
        "          'fantasy_points_ppr_avg':'fantasy_points_ppr-avg'}\n",
        "\n",
        "  full_player_df_current_and_avg = full_player_df_current_and_avg.rename(columns=rename_col_dict)\n",
        "\n",
        "  #filter all opponent defense data to prior to current season and current week\n",
        "\n",
        "  defense_game_df_lead = defense_game_df[((defense_game_df['season'] == current_season) & (defense_game_df['week'] < current_week)) |\n",
        "                                       (defense_game_df['season'] < current_season)].copy()\n",
        "\n",
        "  def_stat_cols = ['team_20_yard_compl_d',\n",
        "       'team_40_yard_compl_d', 'team_60_yard_compl_d',\n",
        "       'team_20_yard_pass_att_d', 'team_40_yard_pass_att_d',\n",
        "       'team_60_yard_pass_att_d', 'team_goal_pass_att_d',\n",
        "       'team_red_pass_att_d', 'team_goal_rush_att_d', 'team_red_rush_att_d',\n",
        "       'team_pass_att_d', 'team_compl_allowed', 'team_rush_att_d',\n",
        "       'pass_tds_allowed', 'rush_tds_allowed', 'passing_yards_allowed',\n",
        "       'rushing_yards_allowed', 'sacks_d', 'yac_allowed', 'ypa_allowed',\n",
        "       'ypc_allowed', 'yac_per_compl_allowed']\n",
        "\n",
        "  #group and average defense stats (should we also include 1) stdev/variances?\n",
        "  defense_game_df_grp = defense_game_df_lead.groupby(['defteam','season'])[def_stat_cols].mean().reset_index()\n",
        "\n",
        "  #merge player data with avg opponent defense stats for the season up to that week\n",
        "\n",
        "  full_player_df_current_avg_def = pd.merge(full_player_df_current_and_avg,defense_game_df_grp,\n",
        "                                          how='left',left_on=['defteam','season'],right_on=['defteam','season'])##need to remove season for weighting of previous seasons\n",
        "\n",
        "  #drop players with 0 as player_id\n",
        "\n",
        "  full_player_df_current_avg_def = full_player_df_current_avg_def[full_player_df_current_avg_def['player_id'] != '0']\n",
        "\n",
        "  #print(list(full_player_df_current_avg_def.columns))\n",
        "\n",
        "  return full_player_df_current_avg_def\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "id": "7iC5Ey99RmEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DWp4k1fgBd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 6"
      ],
      "metadata": {
        "id": "1e0IBxCJO5u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_split_vet_rook_feature_df(full_player_df_current_avg_def, current_season, current_week): #need to reset \n",
        "  #need to clean up columns and split into vets and rookies\n",
        "  print('Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies')\n",
        "  feature_cols = ['game_id','season','game_type','week','gameday','weekday','gametime','div_game',\n",
        "          'roof','surface','temp','wind','teams','gsis_id','position_x','report_status','depth_team','player_id','player_name',\n",
        "          'years_exp','rookie','game_played','game_played_sum','passing_yards_avg','passing_tds_avg','interceptions_avg','sacks_avg',\n",
        "          'sack_fumbles_avg','sack_fumbles_lost_avg','passing_air_yards_avg','compl_passes_20_yds_avg',\n",
        "          'compl_passes_40_yds_avg','ypa_avg','air_ypa_avg','completion_per_avg','passing_td_rate_avg','pass_att_goal_avg','pass_att_red_avg',\n",
        "          'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "          'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg','receptions_avg','targets_avg','receiving_yards_avg','receiving_tds_avg',\n",
        "          'receiving_fumbles_avg','receiving_air_yards_avg','receiving_yards_after_catch_avg','receiving_first_downs_avg','catches_20_yards_avg',\n",
        "          'catches_40_yards_avg','target_share_avg','air_yards_share_avg','goal_targ_team_per_avg','red_targ_team_per_avg',\n",
        "          'wk_pos_rank-fantasy_points-avg','wk_pos_rank-fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_ppr-avg',\n",
        "          'fantasy_points-avg','fantasy_points_halfppr-avg','fantasy_points_ppr-avg','wk_pos_rank-fantasy_points-std','wk_pos_rank-fantasy_points_halfppr-std',\n",
        "          'wk_pos_rank-fantasy_points_ppr-std','fantasy_points-std','fantasy_points_halfppr-std','fantasy_points_ppr-std','team_20_yard_compl_d',\n",
        "          'team_40_yard_compl_d','team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_goal_pass_att_d',\n",
        "          'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed','team_rush_att_d','pass_tds_allowed',\n",
        "          'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed','ypa_allowed','ypc_allowed','yac_per_compl_allowed',\n",
        "           'wk_pos_rank-fantasy_points','wk_pos_rank-fantasy_points_halfppr','wk_pos_rank-fantasy_points_ppr',\n",
        "          'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr']\n",
        "\n",
        "  clean_col_df = full_player_df_current_avg_def[feature_cols]\n",
        "\n",
        "  #need to start encoding categorical variables\n",
        "\n",
        "  #fillna with 0\n",
        "  #clean_col_df.fillna(0,inplace=True)\n",
        "\n",
        "  #to avoid future week data leakage, replace any future weeks:\n",
        "  # 1) weather stats with nan \n",
        "  # 2) weekly depth chart positions with the current depth chart position\n",
        "  # 3) injury report status with nan\n",
        "\n",
        "  clean_data_leak_df = pd.DataFrame()\n",
        "  for p in clean_col_df['player_id'][clean_col_df['season'] == current_season].tolist():\n",
        "    player_df = clean_col_df[clean_col_df['player_id'] == p].copy()\n",
        "    player_df_current_week = player_df[player_df['week'] == current_week].copy()\n",
        "    player_df_past_current_week = player_df[player_df['week'] > current_week].copy()\n",
        "    player_df_past_current_week['temp'] = np.nan\n",
        "    player_df_past_current_week['wind'] = np.nan\n",
        "    player_df_past_current_week['report_status'] = np.nan\n",
        "    if len(player_df_current_week) > 0: #drop any player records that do not have record for current week\n",
        "      player_df_past_current_week['depth_team'] = player_df_current_week['depth_team'].values[0]\n",
        "      new_player_df = pd.concat([player_df_current_week,player_df_past_current_week], join='outer', axis=0)\n",
        "      clean_data_leak_df = pd.concat([clean_data_leak_df,new_player_df], join='outer', axis=0)\n",
        "\n",
        "  #split current season vets from current season rookies\n",
        "  if len(clean_data_leak_df) > 0:\n",
        "    clean_vet_df = clean_data_leak_df[(clean_data_leak_df['rookie'] == 0) | (clean_data_leak_df['season'] < current_season)].copy()\n",
        "    clean_rookie_df = clean_data_leak_df[(clean_data_leak_df['rookie'] == 1) & (clean_data_leak_df['season'] == current_season)].copy()\n",
        "  \n",
        "  return clean_vet_df, clean_rookie_df\n",
        "  "
      ],
      "metadata": {
        "id": "v4SerTI94D_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation Step 7"
      ],
      "metadata": {
        "id": "8pNW6yILPAHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_tweets_create_rookie_draft_profile(clean_rookie_df,combine_data,current_season):\n",
        "\n",
        "\n",
        "  # Create a dictionary containing seasons and tuples containing the day after each draft concludes and the day before the season started\n",
        "  # This time range is relevant for rookies being drafted in fantasy football prior to them actually taking the field (avoiding data leakage)\n",
        "\n",
        "  season_dict = {2018:('2018-04-29','2018-09-05'),\n",
        "               2019:('2019-04-28','2019-09-04'),\n",
        "               2020:('2020-04-26','2020-09-09'),\n",
        "               2021:('2021-05-02','2021-09-08'),\n",
        "               2022:('2022-05-01','2022-09-07')\n",
        "               }\n",
        "\n",
        "  # Creating a list of well-known Fantasy Football Analyst Experts\n",
        "  analysts = ['LateRoundQB','AndyHolloway','jasonffl','FFHitman','EvanSilva','scott_pianowski','daltondeldon','MikeClayNFL','numberFire','LizLoza_FF','andybehrens',\n",
        "            'adamlevitan','adamrank','BrandonFunston','MattHarmon_BYB','TheFantasyPT','kyleynfl','NoisyHuevos','DaveKluge','Ihartitz','friscojosh','KevinColePFF',\n",
        "            'ChrisRaybon','ScottBarrettDFB','BrandonHerFFB','The_Oddsmaker','MikeTagliereNFL','allinkid','dwainmcfarland','justinboone','thepme','Friscojosh','DBro_FFB',\n",
        "            '_nickwhalen','jetpackgalileo']\n",
        "\n",
        "  try: #recommended uploading csv with below name from GitHub repository to local directory, depending on what current season 2018-2022 you want.\n",
        "  #single season takes ~15 mins to scrape\n",
        "    print('Attempting to load {} Tweets from local session CSV'.format(str(current_season)))\n",
        "    tweets_df = pd.read_csv('ff_analyst_tweets_current_season_{}.csv'.format(str(current_season)))\n",
        "\n",
        "  except:\n",
        "    print('Local session CSV not found, scraping Twitter for {} Tweets and saving CSV locally'.format(str(current_season)))\n",
        "    attributes_container = []\n",
        "    for a in tqdm(analysts):\n",
        "      for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:{} since:{} until:{}'.format(a, season_dict[current_season][0],\n",
        "                                                                                               season_dict[current_season][1])).get_items()):\n",
        "        attributes_container.append([tweet.date, tweet.rawContent, a, str(current_season)])\n",
        "    # Creating a dataframe from the tweets list above \n",
        "    tweets_df = pd.DataFrame(attributes_container, columns=[\"Date Created\", \"Tweets\", \"Author\", \"Season\"])\n",
        "    try:\n",
        "      tweets_df.to_csv('ff_analyst_tweets_current_season_{}.csv'.format(str(current_season)), index=False)\n",
        "      print('Tweets succesfully exported to csv')\n",
        "    except:\n",
        "      print('Error while exporting to csv')\n",
        "  \n",
        "  print('Now cleaning tweet data and looking for rookie references')\n",
        "  #light cleaning on tweet data\n",
        "  tweets_df['Tweets'] = tweets_df['Tweets'].str.lower().replace('&amp', '&')\n",
        "\n",
        "  #define custom function to normalize primary names for joining on player across datasets\n",
        "\n",
        "  def normalize_primary_nm(n):\n",
        "    return str(n).lower().strip().replace(\"\\\\\", '').replace(\"'\", '').replace('.', '')\n",
        "\n",
        "  #define custom function to derive different name variations likely in twitter references\n",
        "  #done without regex for clearer visibility into different generated variations\n",
        "\n",
        "  def name_aliases(n):\n",
        "    l = []\n",
        "    n = str(n).lower()\n",
        "    n = n.strip()\n",
        "    n = n.replace(\"\\\\\", '')\n",
        "  \n",
        "    a = n.replace(\"'\", '')\n",
        "    b = n.replace(\"'\", ' ')\n",
        "    c = n.replace('-', '')\n",
        "    d = n.replace('-', ' ')\n",
        "    e = n.replace('.', '')\n",
        "    f = n.replace('.',' ')\n",
        "\n",
        "    for x in [n, a, b, c, d, e, f]:\n",
        "      if x not in l:\n",
        "       l.append(x)\n",
        "\n",
        "    return l\n",
        "  \n",
        "  combine_data['name_norm'] = combine_data['player_name'].map(normalize_primary_nm)\n",
        "\n",
        "  combine_data['aliases'] = combine_data['player_name'].map(name_aliases)\n",
        "\n",
        "  fantasy_positions = ['QB','RB','WR','TE']\n",
        "\n",
        "  # filter to current season, fantasy positions and those who were drafted\n",
        "\n",
        "  current_season_combine_df = combine_data[(combine_data['season'] == current_season) & \n",
        "                                                       (combine_data['pos'].isin(fantasy_positions)) & (~combine_data['draft_year'].isnull())].copy()\n",
        "\n",
        "  # iterate through list of players to find references\n",
        "  player_names_list_of_lists = current_season_combine_df['aliases'].tolist()\n",
        "  player_names = [i for s in player_names_list_of_lists for i in s]\n",
        "  player_names = [n.replace(\"\\\\\", '' ) for n in player_names]\n",
        "  \n",
        "  # filter tweets to those that have player references\n",
        "  tweets_df['references'] = [[p for p in player_names if p in t] for t in tweets_df['Tweets']]\n",
        "  tweets_df['ref_cnt'] = [len(r) for r in tweets_df['references']]\n",
        "\n",
        "  tweets_df_ref = tweets_df[tweets_df['ref_cnt'] > 0].copy()\n",
        "  print('Tweets from {} contain {} tweets with fantasy rookie references'.format(str(current_season),str(len(tweets_df_ref))))\n",
        "\n",
        "  # let's explode the references data\n",
        "  tweets_ref_expl = tweets_df_ref.explode(column='references', ignore_index=True)\n",
        "\n",
        "  # let's explode the draft + conference data by all aliases\n",
        "  current_season_combine_df_expl = current_season_combine_df.explode(column='aliases', ignore_index=True)\n",
        "\n",
        "  #let's merge the draft + conference data with the tweet + ref data\n",
        "  #let's only preserve players with references using inner join\n",
        "  current_season_combine_ref = pd.merge(current_season_combine_df_expl, tweets_ref_expl, how='inner',\n",
        "                                     left_on='aliases', right_on='references')\n",
        "  \n",
        "  print('Now assessing sentiment of rookie tweet references')\n",
        "  # let's use VADER (Valence aware dictionary for sentiment reasoning) sentiment analysis model implemented in NLTK to assess sentiment in each tweet\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  current_season_combine_ref['compound'] = [analyzer.polarity_scores(x)['compound'] for x in current_season_combine_ref['Tweets']]\n",
        "\n",
        "  #let's aggregate our tweet data at the player level and clean up the final result\n",
        "  field_list = ['player_name','draft_year', 'draft_round', 'draft_ovr', 'pos',\n",
        "                'ht', 'wt', 'forty', 'bench', 'vertical', 'broad_jump', 'cone', 'shuttle',\n",
        "                'name_norm','Tweets', 'Author','compound']\n",
        "  field_list_for_agg = ['player_name','draft_year', 'draft_round', 'draft_ovr', 'pos',\n",
        "                'ht', 'wt', 'forty', 'bench', 'vertical', 'broad_jump', 'cone', 'shuttle',\n",
        "                'name_norm']\n",
        "\n",
        "  current_season_combine_ref_fil = current_season_combine_ref[field_list].copy()\n",
        "\n",
        "  #not every rookie completes all combine drills. to get rid of nans lets impute the mean for any NaNs\n",
        "\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['forty'].mean())\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['bench'].mean())\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['vertical'].mean())\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['broad_jump'].mean())\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['cone'].mean())\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil.fillna(current_season_combine_ref_fil['shuttle'].mean())\n",
        "\n",
        "  current_season_combine_ref_fil_imp = current_season_combine_ref_fil_imp.groupby(field_list_for_agg)['compound'].agg(['count','mean','median']).reset_index()\n",
        "\n",
        "  current_season_combine_ref_fil_imp['ht'] = current_season_combine_ref_fil_imp['ht'].fillna('0-0')\n",
        "  current_season_combine_ref_fil_imp['ht_in'] = [str(float(str(h).split('-')[0])*12 + float(str(h).split('-')[1])) if '-' in str(h) else '0' for h in current_season_combine_ref_fil_imp['ht']]\n",
        "  current_season_combine_ref_fil_imp['ht_in'] = current_season_combine_ref_fil_imp['ht_in'].replace('0', np.nan)\n",
        "\n",
        "  final_draft_profile_cols = ['player_name','draft_year', 'draft_round', 'draft_ovr', 'pos',\n",
        "                'ht_in', 'wt', 'forty', 'bench', 'vertical', 'broad_jump', 'cone', 'shuttle',\n",
        "                'name_norm','count','mean','median']\n",
        "\n",
        "  final_draft_profile = current_season_combine_ref_fil_imp[final_draft_profile_cols].copy()\n",
        "\n",
        "  #let's give our expert sentiment fields more meaningful names\n",
        "\n",
        "  final_draft_profile = final_draft_profile.rename(columns={'count':'tweet_ref_cnt','mean':'mean_compound_sentiment','media':'median_compound_sentiment'})\n",
        "\n",
        "  #let's merge this draft profile with our clean rookie stat df\n",
        "\n",
        "  clean_rookie_df['player_name_norm'] = clean_rookie_df['player_name'].map(normalize_primary_nm)\n",
        "\n",
        "\n",
        "\n",
        "  final_draft_profile_clean_rookie_df = pd.merge(final_draft_profile,clean_rookie_df,how='inner',left_on=['name_norm'],\n",
        "                                                 right_on=['player_name_norm'])\n",
        "  \n",
        "  final_draft_profile_clean_rookie_df = final_draft_profile_clean_rookie_df.drop_duplicates()\n",
        "  \n",
        "  return final_draft_profile_clean_rookie_df\n"
      ],
      "metadata": {
        "id": "Np6yeElH7iAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine_data[combine_data['ht'].isnull()]"
      ],
      "metadata": {
        "id": "FYLBaXvS-yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOYjd2mlP2xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9PmzewxQDpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "As4DmM0RVgzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Data Extraction and Preparation Portion of Pipeline"
      ],
      "metadata": {
        "id": "hefVuPZX42nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#seasons = [2022]\n",
        "#number of seasons to pull\n",
        "seasons = [2020,2021,2022]\n",
        "\n",
        "#relevant columns to extract from nfl_data_py\n",
        "pbp_cols = ['play_id', 'game_id','home_team', 'away_team', 'season_type', 'week','posteam', 'posteam_type','defteam','side_of_field', 'yardline_100','goal_to_go',\n",
        "            'yards_gained','play_type', 'qb_dropback', 'qb_kneel', 'qb_spike', 'qb_scramble','pass_length', 'pass_location', 'air_yards','sack','yards_after_catch', 'run_location', 'run_gap',\n",
        "            'td_team', 'td_player_name','td_player_id','rush_attempt', 'pass_attempt','touchdown', 'pass_touchdown','rush_touchdown', 'return_touchdown',\n",
        "            'fumble','complete_pass', 'passer_player_id', 'passer_player_name', 'passing_yards', 'receiver_player_id', 'receiver_player_name', 'receiving_yards', \n",
        "            'rusher_player_id', 'rusher_player_name', 'rushing_yards','fumbled_1_player_id', 'fumbled_1_player_name','success','pass','rush','play', 'passer_id',\n",
        "            'rusher_id', 'receiver_id','fantasy_player_name', 'fantasy_player_id']\n",
        "\n",
        "wk_cols = ['player_id', 'player_name', 'player_display_name', 'position','recent_team', 'season', 'week', 'season_type', 'completions', 'attempts',\n",
        "           'passing_yards', 'passing_tds', 'interceptions', 'sacks', 'sack_yards','sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards', 'passing_yards_after_catch', \n",
        "           'carries','rushing_yards', 'rushing_tds', 'rushing_fumbles', 'rushing_fumbles_lost', 'rushing_first_downs','receptions', 'targets', 'receiving_yards',\n",
        "           'receiving_tds', 'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_air_yards', 'receiving_yards_after_catch',\n",
        "           'receiving_first_downs', 'receiving_2pt_conversions','target_share','air_yards_share','fantasy_points', 'fantasy_points_ppr']\n",
        "\n",
        "#this will be updated to current week when this is productionized. step 5-7 only run for current season + current week here\n",
        "current_season = 2022\n",
        "current_week = 12\n",
        "\n",
        "#Step 1: Extract various DFs from nfl_data_py library for seasons specified in seasons variable\n",
        "pbp_data, week_data, sched_data, rosters_data, injuries_data, combine_data, depth_data, latest_season = extract_all_season_data_dfs(seasons, pbp_cols, wk_cols)\n",
        "\n",
        "#Step 2: Extract and aggregate play by play data to create additional player counting stats and opposing defense stats\n",
        "#Step 2.5: Clean and split into relevant game play category DFs (passing, rushing, receiving, total offense and total defense)\n",
        "player_game_df_pass, player_game_df_rush, player_game_df_receive, offense_game_df, defense_game_df = aggregate_pbp_data(additional_pbp_counting_stats(pbp_data))\n",
        "\n",
        "#Step 3: Create a weekly stat DF for each player based on their stats in each game play category as well as total offense and total defense and week pos rankings\n",
        "weekly_mas_df, weekly_pos_rankings = create_master_weekly_player_data(week_data, player_game_df_pass, player_game_df_rush, player_game_df_receive, offense_game_df, defense_game_df)\n",
        "\n",
        "#Step 4: Create season, week and player-level DF using schedules, rosters, depth charts and injury reports\n",
        "sched_dep_ros_inj, team_df = create_schedule_df(sched_data, rosters_data, depth_data, injuries_data)\n",
        "\n",
        "#Step 5: Aggregate player and opposing team defense average stats for past seasons and current season up to not including current week into one player df for the current week and weeks through end of season\n",
        "full_player_df_current_avg_def = agg_player_stats_weeks_leading_up(sched_dep_ros_inj, weekly_mas_df, weekly_pos_rankings,\n",
        "                                                                   defense_game_df, current_season=current_season, current_week=current_week)\n",
        "\n",
        "#Step 6: Clean player df in preparation for supervised learning, limit data leakage, split into veterns versus rookies\n",
        "clean_vet_df, clean_rookie_df = clean_and_split_vet_rook_feature_df(full_player_df_current_avg_def, current_season=current_season, current_week=current_week)\n",
        "\n",
        "#Step 7: Create rookie draft profile for current season rookies to add as features in lieu of or combined with available current season data\n",
        "\n",
        "final_draft_profile_clean_rookie_df = scrape_tweets_create_rookie_draft_profile(clean_rookie_df,combine_data,current_season=current_season)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzfN6BKTpDeQ",
        "outputId": "8362f49b-955e-4889-fc05-72cde3b6e3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now extracting all dataframes from nfl_data_py\n",
            "2020 done.\n",
            "2021 done.\n",
            "2022 done.\n",
            "Now using play-by-play data to identify specific fantasy relevant plays\n",
            "Now aggregating all play-by-play data to week level and breaking into play type categories (pass, rush, receive, total offense, total defense)\n",
            "Now creating weekly historical positional rankings by each score format\n",
            "Now combining all player stats data at the player and week level\n",
            "Now creating a dataframe containing schedule, team roster, depth chart and injuries at the player and week level\n",
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 12\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list(full_player_df_current_avg_def.columns)"
      ],
      "metadata": {
        "id": "dKqFIWCXGLCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_player_df_current_avg_def.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "rCn3TZcpefdc",
        "outputId": "16a2a2c4-0817-4843-d85d-64da17254456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           game_id  season game_type  week     gameday   weekday gametime  \\\n",
              "0  2022_12_BUF_DET    2022       REG    12  2022-11-24  Thursday    12:30   \n",
              "1   2022_13_BUF_NE    2022       REG    13  2022-12-01  Thursday    20:15   \n",
              "2  2022_14_NYJ_BUF    2022       REG    14  2022-12-11    Sunday    13:00   \n",
              "3  2022_15_MIA_BUF    2022       REG    15  2022-12-17  Saturday    20:15   \n",
              "4  2022_16_BUF_CHI    2022       REG    16  2022-12-24  Saturday    13:00   \n",
              "\n",
              "   div_game      roof    surface  ...  team_rush_att_d  pass_tds_allowed  \\\n",
              "0         0      dome  fieldturf  ...        28.500000          1.600000   \n",
              "1         1  outdoors  fieldturf  ...        24.900000          1.200000   \n",
              "2         1  outdoors     a_turf  ...        26.500000          0.900000   \n",
              "3         1  outdoors     a_turf  ...        24.500000          1.600000   \n",
              "4         0  outdoors      grass  ...        29.454545          1.090909   \n",
              "\n",
              "  rush_tds_allowed passing_yards_allowed rushing_yards_allowed   sacks_d  \\\n",
              "0         1.700000            273.600000            155.100000  1.700000   \n",
              "1         0.400000            211.600000            115.000000  3.600000   \n",
              "2         1.000000            221.100000            110.700000  3.200000   \n",
              "3         1.200000            253.900000            118.400000  2.000000   \n",
              "4         1.636364            205.454545            143.909091  1.363636   \n",
              "\n",
              "  yac_allowed ypa_allowed ypc_allowed yac_per_compl_allowed  \n",
              "0  106.600000    7.838479    5.288965              4.980590  \n",
              "1   98.500000    5.821589    4.408976              5.317196  \n",
              "2  105.300000    6.113083    4.129911              4.857339  \n",
              "3  123.800000    7.105898    4.758190              5.445364  \n",
              "4  101.181818    7.166096    4.770511              5.638532  \n",
              "\n",
              "[5 rows x 156 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fcb3a94-660b-4556-a62f-974b96bf117b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>game_id</th>\n",
              "      <th>season</th>\n",
              "      <th>game_type</th>\n",
              "      <th>week</th>\n",
              "      <th>gameday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>gametime</th>\n",
              "      <th>div_game</th>\n",
              "      <th>roof</th>\n",
              "      <th>surface</th>\n",
              "      <th>...</th>\n",
              "      <th>team_rush_att_d</th>\n",
              "      <th>pass_tds_allowed</th>\n",
              "      <th>rush_tds_allowed</th>\n",
              "      <th>passing_yards_allowed</th>\n",
              "      <th>rushing_yards_allowed</th>\n",
              "      <th>sacks_d</th>\n",
              "      <th>yac_allowed</th>\n",
              "      <th>ypa_allowed</th>\n",
              "      <th>ypc_allowed</th>\n",
              "      <th>yac_per_compl_allowed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022_12_BUF_DET</td>\n",
              "      <td>2022</td>\n",
              "      <td>REG</td>\n",
              "      <td>12</td>\n",
              "      <td>2022-11-24</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>12:30</td>\n",
              "      <td>0</td>\n",
              "      <td>dome</td>\n",
              "      <td>fieldturf</td>\n",
              "      <td>...</td>\n",
              "      <td>28.500000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>273.600000</td>\n",
              "      <td>155.100000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>106.600000</td>\n",
              "      <td>7.838479</td>\n",
              "      <td>5.288965</td>\n",
              "      <td>4.980590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022_13_BUF_NE</td>\n",
              "      <td>2022</td>\n",
              "      <td>REG</td>\n",
              "      <td>13</td>\n",
              "      <td>2022-12-01</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>20:15</td>\n",
              "      <td>1</td>\n",
              "      <td>outdoors</td>\n",
              "      <td>fieldturf</td>\n",
              "      <td>...</td>\n",
              "      <td>24.900000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>211.600000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>5.821589</td>\n",
              "      <td>4.408976</td>\n",
              "      <td>5.317196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022_14_NYJ_BUF</td>\n",
              "      <td>2022</td>\n",
              "      <td>REG</td>\n",
              "      <td>14</td>\n",
              "      <td>2022-12-11</td>\n",
              "      <td>Sunday</td>\n",
              "      <td>13:00</td>\n",
              "      <td>1</td>\n",
              "      <td>outdoors</td>\n",
              "      <td>a_turf</td>\n",
              "      <td>...</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>221.100000</td>\n",
              "      <td>110.700000</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>105.300000</td>\n",
              "      <td>6.113083</td>\n",
              "      <td>4.129911</td>\n",
              "      <td>4.857339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022_15_MIA_BUF</td>\n",
              "      <td>2022</td>\n",
              "      <td>REG</td>\n",
              "      <td>15</td>\n",
              "      <td>2022-12-17</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>20:15</td>\n",
              "      <td>1</td>\n",
              "      <td>outdoors</td>\n",
              "      <td>a_turf</td>\n",
              "      <td>...</td>\n",
              "      <td>24.500000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>253.900000</td>\n",
              "      <td>118.400000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>123.800000</td>\n",
              "      <td>7.105898</td>\n",
              "      <td>4.758190</td>\n",
              "      <td>5.445364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022_16_BUF_CHI</td>\n",
              "      <td>2022</td>\n",
              "      <td>REG</td>\n",
              "      <td>16</td>\n",
              "      <td>2022-12-24</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>13:00</td>\n",
              "      <td>0</td>\n",
              "      <td>outdoors</td>\n",
              "      <td>grass</td>\n",
              "      <td>...</td>\n",
              "      <td>29.454545</td>\n",
              "      <td>1.090909</td>\n",
              "      <td>1.636364</td>\n",
              "      <td>205.454545</td>\n",
              "      <td>143.909091</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>101.181818</td>\n",
              "      <td>7.166096</td>\n",
              "      <td>4.770511</td>\n",
              "      <td>5.638532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  156 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fcb3a94-660b-4556-a62f-974b96bf117b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fcb3a94-660b-4556-a62f-974b96bf117b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fcb3a94-660b-4556-a62f-974b96bf117b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Portion of Pipeline"
      ],
      "metadata": {
        "id": "pCKkkRoTHiVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's store all columns relevant to each position for regression modeling purposes as a dict\n",
        "#this has been updated as part of model tuning and EDA\n",
        "\n",
        "\n",
        "reg_pos_col_dict = {\n",
        "'QB':['season','week','player_id','player_name','report_status','depth_team','div_game','years_exp','game_played_sum',\n",
        "      'passing_yards_avg','passing_tds_avg','interceptions_avg','sacks_avg',\n",
        "      'sack_fumbles_avg','sack_fumbles_lost_avg','passing_air_yards_avg','compl_passes_20_yds_avg',\n",
        "      'compl_passes_40_yds_avg','ypa_avg','air_ypa_avg','completion_per_avg','passing_td_rate_avg','pass_att_goal_avg','pass_att_red_avg',\n",
        "      'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "      'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg',\n",
        "      'team_20_yard_compl_d','team_40_yard_compl_d','team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_goal_pass_att_d',\n",
        "      'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed','team_rush_att_d','pass_tds_allowed',\n",
        "      'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed','ypa_allowed','ypc_allowed','yac_per_compl_allowed',\n",
        "      'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr'],\n",
        "\n",
        "'RB':['season','week','player_id','player_name','report_status','depth_team','div_game','years_exp','game_played_sum',\n",
        "      'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "      'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg','receptions_avg','targets_avg','receiving_yards_avg','receiving_tds_avg',\n",
        "      'receiving_fumbles_avg','receiving_air_yards_avg','receiving_yards_after_catch_avg','receiving_first_downs_avg','catches_20_yards_avg',\n",
        "      'catches_40_yards_avg','target_share_avg','air_yards_share_avg','goal_targ_team_per_avg','red_targ_team_per_avg','team_goal_pass_att_d',\n",
        "      'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_rush_att_d','pass_tds_allowed',\n",
        "      'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','yac_allowed','ypc_allowed',\n",
        "      'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr'],\n",
        "\n",
        "'WR':['season','week','player_id','player_name','report_status','depth_team','div_game','years_exp','game_played_sum',\n",
        "      'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "      'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg','receptions_avg','targets_avg','receiving_yards_avg','receiving_tds_avg',\n",
        "      'receiving_fumbles_avg','receiving_air_yards_avg','receiving_yards_after_catch_avg','receiving_first_downs_avg','catches_20_yards_avg',\n",
        "      'catches_40_yards_avg','target_share_avg','air_yards_share_avg','goal_targ_team_per_avg','red_targ_team_per_avg','team_20_yard_compl_d',\n",
        "      'team_40_yard_compl_d','team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_goal_pass_att_d',\n",
        "      'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed','team_rush_att_d','pass_tds_allowed',\n",
        "      'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed','ypa_allowed','ypc_allowed','yac_per_compl_allowed',\n",
        "      'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr'],\n",
        "\n",
        "'TE':['season','week','player_id','player_name','report_status','depth_team','div_game','years_exp','game_played_sum',\n",
        "      'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "      'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg','receptions_avg','targets_avg','receiving_yards_avg','receiving_tds_avg',\n",
        "      'receiving_fumbles_avg','receiving_air_yards_avg','receiving_yards_after_catch_avg','receiving_first_downs_avg','catches_20_yards_avg',\n",
        "      'catches_40_yards_avg','target_share_avg','air_yards_share_avg','goal_targ_team_per_avg','red_targ_team_per_avg','team_20_yard_compl_d',\n",
        "      'team_40_yard_compl_d','team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_goal_pass_att_d',\n",
        "      'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed','team_rush_att_d','pass_tds_allowed',\n",
        "      'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed','ypa_allowed','ypc_allowed','yac_per_compl_allowed',\n",
        "      'fantasy_points','fantasy_points_halfppr','fantasy_points_ppr'],\n",
        "\n",
        "'ROOKIE':['draft_round','draft_ovr','ht_in',\n",
        "       'wt','forty','bench','vertical','broad_jump','cone','shuttle',\n",
        "       'name_norm','tweet_ref_cnt','mean_compound_sentiment'],\n",
        "\n",
        "'POS_RANK':['wk_pos_rank-fantasy_points-avg',\n",
        "      'wk_pos_rank-fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_ppr-avg',\n",
        "      'fantasy_points-avg','fantasy_points_halfppr-avg','fantasy_points_ppr-avg'],\n",
        "\n",
        "'STDEV':['wk_pos_rank-fantasy_points-std','wk_pos_rank-fantasy_points_halfppr-std',\n",
        "      'wk_pos_rank-fantasy_points_ppr-std','fantasy_points-std','fantasy_points_halfppr-std',\n",
        "      'fantasy_points_ppr-std']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#dict to set depth chart values for filtering\n",
        "pos_depth_chart_dict = {'QB':1,'RB':2,'WR':2,'TE':1}\n",
        "\n",
        "#set week cutoff to allow for aggregation of results in that season\n",
        "week_cutoff = 3"
      ],
      "metadata": {
        "id": "WORg2xJCHn7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Setup Step 1 - Create DF containing all current week features for regression"
      ],
      "metadata": {
        "id": "-0fWPmuIPUjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to return a DF containing all rows with current season and current week features\n",
        "# this means we are trying to make predictions using all game data available up to that point\n",
        "# week_cutoff is variable to only begin predictions in n week\n",
        "# games_started_cutoff is variable to only consider players who have played in a min of n games so far that season\n",
        "\n",
        "## need to run Extraction and Data Preparation Pipeline First\n",
        "def create_all_current_week_df(sched_dep_ros_inj,weekly_mas_df,defense_game_df,combine_data,\n",
        "                               seasons=seasons,week_cutoff=week_cutoff,games_played_cutoff=2):\n",
        "  all_current_week_df_veteran = pd.DataFrame()\n",
        "  all_current_week_df_rookie = pd.DataFrame()\n",
        "\n",
        "  for s in seasons:\n",
        "    try:\n",
        "      # attempt to read in these DFs locally (takes ~2 seconds per week to compile DF)\n",
        "\n",
        "      all_current_week_df_season_v = pd.read_csv('all_current_week_df_veteran_{}.csv'.format(str(s)))\n",
        "      all_current_week_df_veteran = pd.concat([all_current_week_df_veteran,all_current_week_df_season_v],join='outer',axis=0)\n",
        "\n",
        "      all_current_week_df_season_r = pd.read_csv('all_current_week_df_rookie_{}.csv'.format(str(s)))\n",
        "      all_current_week_df_rookie = pd.concat([all_current_week_df_rookie,all_current_week_df_season_r],join='outer',axis=0)\n",
        "\n",
        "    except:\n",
        "      week_list = [x for x in list(sched_dep_ros_inj[sched_dep_ros_inj['season']==s]['week'].unique()) if x >= week_cutoff]\n",
        "      print('generating feature dfs for the following weeks:',week_list)\n",
        "      for w in tqdm(week_list):\n",
        "        #execute aggregation up to current week function from data prep function\n",
        "        full_player_df_current_avg_def = agg_player_stats_weeks_leading_up(sched_dep_ros_inj, weekly_mas_df, weekly_pos_rankings,\n",
        "                                                                           defense_game_df, current_season=s, current_week=w)\n",
        "        #only retain s season and w week row for each player (current season and current week at each loop)\n",
        "        full_player_df_current_avg_def = full_player_df_current_avg_def[((full_player_df_current_avg_def['week']==w) &\n",
        "                                                                         (full_player_df_current_avg_def['season']==s) &\n",
        "                                                                         (full_player_df_current_avg_def['week']>=week_cutoff) &\n",
        "                                                                         (full_player_df_current_avg_def['game_played_sum']>=games_played_cutoff)\n",
        "                                                                         )].copy()\n",
        "        clean_vet_df, clean_rookie_df = clean_and_split_vet_rook_feature_df(full_player_df_current_avg_def, current_season=s, current_week=w)\n",
        "        final_draft_profile_clean_rookie_df = scrape_tweets_create_rookie_draft_profile(clean_rookie_df,combine_data,current_season=s)\n",
        "\n",
        "\n",
        "        all_current_week_df_veteran = pd.concat([all_current_week_df_veteran,clean_vet_df],join='outer',axis=0)\n",
        "        all_current_week_df_rookie = pd.concat([all_current_week_df_rookie,final_draft_profile_clean_rookie_df],join='outer',axis=0)\n",
        "      \n",
        "      all_current_week_df_veteran.to_csv('all_current_week_df_veteran_{}.csv'.format(str(s)))\n",
        "      all_current_week_df_rookie.to_csv('all_current_week_df_rookie_{}.csv'.format(str(s)))\n",
        "\n",
        "  return all_current_week_df_veteran, all_current_week_df_rookie\n"
      ],
      "metadata": {
        "id": "AZp0Ml6pPTYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Setup Step 2 - Create positional DFs by experience (vet and rookie)"
      ],
      "metadata": {
        "id": "tVM-Xc1Hrs7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that takes vet and rookie dfs and returns clean positional vet and rookie dfs stored in a dictionary by vet/rookie and position\n",
        "# also takes in two dicts, one to clean cols and one to set depth chart thresholds\n",
        "# can be used for single current week or all current week dfs\n",
        "\n",
        "def clean_and_split_by_pos_for_model(current_week_df_veteran, current_week_df_rookie, reg_pos_col_dict=reg_pos_col_dict,\n",
        "                                     pos_depth_chart_dict=pos_depth_chart_dict):\n",
        "  \n",
        "  clean_pos_results_dict = {}\n",
        "\n",
        "  ###new###\n",
        "  clean_pos_results_dict['vet'] = {}\n",
        "  clean_pos_results_dict['rookie'] = {}\n",
        "  ###end new###\n",
        "  fantasy_positions = ['QB','RB','WR','TE']\n",
        "  current_week_df_rookie = current_week_df_rookie.rename(columns={'player_name_norm':'player_name'})\n",
        "  for p in fantasy_positions:\n",
        "    \n",
        "    # filter by position\n",
        "    fil_vet = current_week_df_veteran[current_week_df_veteran['position_x']==p].copy()\n",
        "    fil_rook = current_week_df_rookie[current_week_df_rookie['position_x']==p].copy()\n",
        "\n",
        "    # clean cols using passed dict\n",
        "    fil_vet = fil_vet[reg_pos_col_dict[p]]\n",
        "    fil_rook = fil_rook[reg_pos_col_dict[p]+reg_pos_col_dict['ROOKIE']]\n",
        "\n",
        "    # filter by depth chart thresholds -- ensures only fantasy relevant players included\n",
        "    fil_vet['depth_team'] = fil_vet['depth_team'].astype(int)\n",
        "    fil_rook['depth_team'] = fil_rook['depth_team'].astype(int)\n",
        "    fil_vet = fil_vet[fil_vet['depth_team'] <= pos_depth_chart_dict[p]]\n",
        "    fil_rook = fil_rook[fil_rook['depth_team'] <= pos_depth_chart_dict[p]]\n",
        "\n",
        "    # set indexes\n",
        "    fil_vet = fil_vet.reset_index().set_index(keys=['season','week','player_id','player_name','report_status'])\n",
        "    fil_rook = fil_rook.reset_index().set_index(keys=['season','week','player_id','player_name','report_status'])\n",
        "\n",
        "    # fillna with 0\n",
        "\n",
        "    fil_vet = fil_vet.fillna(0)\n",
        "    fil_rook = fil_vet.fillna(0)\n",
        "\n",
        "    # drop extra index\n",
        "    fil_vet = fil_vet.drop(labels='index', axis=1)\n",
        "    fil_rook = fil_rook.drop(labels='index', axis=1)\n",
        "\n",
        "    ###updated###\n",
        "    clean_pos_results_dict['vet'][p] = fil_vet\n",
        "    clean_pos_results_dict['rookie'][p] = fil_rook\n",
        "    ###end updated###\n",
        "\n",
        "  return clean_pos_results_dict"
      ],
      "metadata": {
        "id": "gWxBlzP6g5Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Setup Step 3 - Create feature df with boom-bust and buy-sell-hold labels for Multi Layer Perceptron classification training"
      ],
      "metadata": {
        "id": "5NR4279nNjyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def create_boom_bust_buy_sell_dfs(current_week_df_veteran, current_week_df_rookie, reg_pos_col_dict=reg_pos_col_dict,\n",
        "                                     pos_depth_chart_dict=pos_depth_chart_dict, boom_thres=7, bust_thres=-0.01, pos_rank_change_buy=-20,\n",
        "                                 pos_rank_change_sell=20,seasons=seasons):\n",
        "  \n",
        "  scoring_formats = ['fantasy_points','fantasy_points_halfppr','fantasy_points_ppr']\n",
        "\n",
        "  actual_cols = ['fantasy_points','fantasy_points_halfppr','fantasy_points_ppr',\n",
        "                 'wk_pos_rank-fantasy_points','wk_pos_rank-fantasy_points_halfppr',\n",
        "                'wk_pos_rank-fantasy_points_ppr']\n",
        "  \n",
        "  lead_avg_cols = ['wk_pos_rank-fantasy_points-avg',\n",
        "      'wk_pos_rank-fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_ppr-avg',\n",
        "      'fantasy_points-avg','fantasy_points_halfppr-avg','fantasy_points_ppr-avg']\n",
        "\n",
        "  lead_stdev_cols = ['wk_pos_rank-fantasy_points-std','wk_pos_rank-fantasy_points_halfppr-std',\n",
        "      'wk_pos_rank-fantasy_points_ppr-std','fantasy_points-std','fantasy_points_halfppr-std',\n",
        "      'fantasy_points_ppr-std']\n",
        "\n",
        "  # filter by depth chart thresholds -- ensures only fantasy relevant players included\n",
        "  current_week_df_veteran['depth_team'] = current_week_df_veteran['depth_team'].astype(int)\n",
        "  current_week_df_rookie['depth_team'] = current_week_df_rookie['depth_team'].astype(int)\n",
        "\n",
        "  current_week_df_veteran['depth_team_check'] = current_week_df_veteran['position_x'].map(pos_depth_chart_dict)\n",
        "  current_week_df_rookie['depth_team_check'] = current_week_df_rookie['position_x'].map(pos_depth_chart_dict)\n",
        "\n",
        "  vet_fil = current_week_df_veteran[current_week_df_veteran['depth_team'] <= current_week_df_veteran['depth_team_check']].copy()\n",
        "  rook_fil = current_week_df_rookie[current_week_df_rookie['depth_team'] <= current_week_df_rookie['depth_team_check']].copy()\n",
        "\n",
        "  #key error n_stdevs-fantasy_points\n",
        "\n",
        "  # calculate z scores for each current week observation\n",
        "\n",
        "  for a in actual_cols:\n",
        "    for df in [vet_fil, rook_fil]:\n",
        "      df['n_stdevs-{}'.format(a)] = (df[a]/df['{}-avg'.format(a)])/(\n",
        "          df['{}-std'.format(a)])\n",
        "\n",
        "  # label booms or busts depending on z scores being above or below set thresholds   \n",
        "  for df in [vet_fil, rook_fil]:\n",
        "    for s in scoring_formats:\n",
        "      df['{}-boom'.format(s)] = np.where(df['n_stdevs-{}'.format(s)] >= boom_thres, 1, 0)\n",
        "      df['{}-bust'.format(s)] = np.where(df['n_stdevs-{}'.format(s)] < bust_thres, 1, 0)\n",
        "\n",
        "  # to label buy-sell we need to calculate avg current+future weeks pos ranking\n",
        "  buy_sell_labels_df = pd.DataFrame()\n",
        "  for df in [vet_fil, rook_fil]:\n",
        "    for s in seasons:\n",
        "      season_df = df[df['season'] == s].copy()\n",
        "      min_week = season_df['week'].min()\n",
        "      max_week = season_df['week'].max()\n",
        "      for w in np.arange(min_week, max_week+1, 1):\n",
        "        #print('Labeling the Buy Sell Data for {} Season, Week {}'.format(str(s),str(w)))\n",
        "        current_df = season_df[season_df['week'] >= w].copy()\n",
        "        current_df_trim = current_df[['player_id','season','week','wk_pos_rank-fantasy_points-avg',\n",
        "              'wk_pos_rank-fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_ppr-avg',\n",
        "              'wk_pos_rank-fantasy_points','wk_pos_rank-fantasy_points_halfppr',\n",
        "                'wk_pos_rank-fantasy_points_ppr']].copy()\n",
        "\n",
        "        grouped_df = current_df_trim.groupby(['player_id','season','week','wk_pos_rank-fantasy_points-avg',\n",
        "              'wk_pos_rank-fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_ppr-avg'])[['wk_pos_rank-fantasy_points',\n",
        "                                                                                              'wk_pos_rank-fantasy_points_halfppr',\n",
        "                                                                                              'wk_pos_rank-fantasy_points_ppr']].mean().reset_index()\n",
        "        grouped_df = grouped_df.drop_duplicates()\n",
        "\n",
        "        new_col_names = {'wk_pos_rank-fantasy_points':'remain_pos_rank-fantasy_points',\n",
        "                         'wk_pos_rank-fantasy_points_halfppr':'remain_pos_rank-fantasy_points_halfppr',\n",
        "                         'wk_pos_rank-fantasy_points_ppr':'remain_pos_rank-fantasy_points_ppr'}\n",
        "\n",
        "        grouped_df = grouped_df.rename(columns=new_col_names)\n",
        "\n",
        "        # let's calculate the difference in average player positions to identify buy or sell (negative change is good in this case, lower ranks are better)\n",
        "\n",
        "        grouped_df['sell'] = np.where((grouped_df['remain_pos_rank-fantasy_points_halfppr']-\n",
        "                                      grouped_df['wk_pos_rank-fantasy_points_halfppr-avg']) >= pos_rank_change_sell,1,0)\n",
        "        \n",
        "        grouped_df['buy'] = np.where((grouped_df['remain_pos_rank-fantasy_points_halfppr']-\n",
        "                                      grouped_df['wk_pos_rank-fantasy_points_halfppr-avg']) < pos_rank_change_buy,1,0) \n",
        "\n",
        "        buy_sell_labels_df = pd.concat([buy_sell_labels_df,grouped_df], join='outer',axis=0)\n",
        "  \n",
        "  bb_label_cols = ['player_id','season','week','fantasy_points_halfppr-boom','fantasy_points_halfppr-bust']\n",
        "  bs_label_cols = ['player_id','season','week','buy','sell']\n",
        "\n",
        "  vet_labels = vet_fil[bb_label_cols].copy()\n",
        "  rook_labels = rook_fil[bb_label_cols].copy()\n",
        "  \n",
        "  vet_rook_labels = pd.concat([vet_labels,rook_labels],join='outer',axis=0)  \n",
        "\n",
        "  vet_rook_labels = vet_rook_labels.drop_duplicates()\n",
        "  #vet_rook_labels = vet_labels.set_index(keys=['player_id','season','week'])\n",
        "\n",
        "  buy_sell_labels_df_trim = buy_sell_labels_df[bs_label_cols].copy()\n",
        "\n",
        "  final_labels_df = pd.merge(vet_rook_labels,buy_sell_labels_df_trim,how='inner',left_on=['player_id','season','week'],\n",
        "                             right_on=['player_id','season','week'])\n",
        "\n",
        "  final_labels_df = final_labels_df.drop_duplicates()\n",
        "\n",
        "  final_labels_df['fantasy_points_halfppr-bust'] = np.where(final_labels_df['fantasy_points_halfppr-bust'] == 1, -1, 0)\n",
        "  final_labels_df['sell'] = np.where(final_labels_df['sell'] == 1, -1, 0)\n",
        "\n",
        "  final_labels_df['boom_or_bust'] = np.where(final_labels_df['fantasy_points_halfppr-boom'] == 1, 1, final_labels_df['fantasy_points_halfppr-bust'])\n",
        "  final_labels_df['buy_or_sell'] = np.where(final_labels_df['buy'] == 1, 1, final_labels_df['sell'])\n",
        "\n",
        "  return final_labels_df\n",
        "\n"
      ],
      "metadata": {
        "id": "5ShUegfMSh91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list(all_current_week_df_veteran.columns)"
      ],
      "metadata": {
        "id": "rcB_nm7E-dd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2AMkQ1ss-oAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Setup Step 4 - Create Clean Labeled Feature DFs for Multilayer Perceptron Classifiers for boom-bust and buy-sell"
      ],
      "metadata": {
        "id": "4I1zR_XvyBhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clean_boom_bust_buy_sell_df(current_week_df_veteran, current_week_df_rookie, final_labels_df, seasons=seasons):\n",
        "  \n",
        "  # first need to get remaining season average defense stats for buy_sell and offensive features\n",
        "  index_cols = ['player_id','player_name','season','week']\n",
        "\n",
        "  off_avg_cols = ['position_x','depth_team','years_exp','rookie','passing_yards_avg','passing_tds_avg','interceptions_avg','sacks_avg',\n",
        "          'sack_fumbles_avg','sack_fumbles_lost_avg','passing_air_yards_avg','compl_passes_20_yds_avg',\n",
        "          'compl_passes_40_yds_avg','ypa_avg','air_ypa_avg','completion_per_avg','passing_td_rate_avg','pass_att_goal_avg','pass_att_red_avg',\n",
        "          'carries_avg','rushing_yards_avg','rushing_tds_avg','rushing_fumbles_avg','rushing_first_downs_avg','rush_20_yds_avg','rush_40_yds_avg',\n",
        "          'goal_rush_team_per_avg','red_rush_team_per_avg','tot_rush_team_per_avg','receptions_avg','targets_avg','receiving_yards_avg','receiving_tds_avg',\n",
        "          'receiving_fumbles_avg','receiving_air_yards_avg','receiving_yards_after_catch_avg','receiving_first_downs_avg','catches_20_yards_avg',\n",
        "          'catches_40_yards_avg','target_share_avg','air_yards_share_avg','goal_targ_team_per_avg','red_targ_team_per_avg',\n",
        "          'wk_pos_rank-fantasy_points_halfppr-avg','fantasy_points_halfppr-avg','wk_pos_rank-fantasy_points_halfppr-std',\n",
        "          'fantasy_points_halfppr-std']\n",
        "\n",
        "  def_avg_cols = ['team_20_yard_compl_d','team_40_yard_compl_d','team_20_yard_pass_att_d','team_40_yard_pass_att_d','team_goal_pass_att_d',\n",
        "          'team_red_pass_att_d','team_goal_rush_att_d','team_red_rush_att_d','team_pass_att_d','team_compl_allowed','team_rush_att_d','pass_tds_allowed',\n",
        "          'rush_tds_allowed','passing_yards_allowed','rushing_yards_allowed','sacks_d','yac_allowed','ypa_allowed','ypc_allowed','yac_per_compl_allowed']\n",
        "\n",
        "\n",
        "  pos_dict = {'QB':0,'RB':1,'WR':2,'TE':3}\n",
        "\n",
        "  current_week_df_rookie = current_week_df_rookie.rename(columns={'player_name_norm':'player_name'})\n",
        "\n",
        "  vet_fil = current_week_df_veteran[index_cols + off_avg_cols + def_avg_cols].copy()\n",
        "\n",
        "  #lets prepare all the dfs for boom bust and buy sell\n",
        "\n",
        "  rook_fil = current_week_df_rookie[index_cols + off_avg_cols + def_avg_cols].copy()\n",
        "\n",
        "  bb_labels_fil = final_labels_df[['player_id','season','week','boom_or_bust']].copy()\n",
        "\n",
        "  bs_labels_fil = final_labels_df[['player_id','season','week','buy_or_sell']].copy()\n",
        "\n",
        "  full_df = pd.concat([vet_fil,rook_fil],join='outer',axis=0)\n",
        "\n",
        "  full_df = full_df.drop_duplicates()\n",
        "\n",
        "  full_df['position_x'] = full_df['position_x'].map(pos_dict)\n",
        "\n",
        "  #create boom bust df with features up until the current week on, joined with the labels\n",
        "\n",
        "  bb_df = pd.merge(full_df,bb_labels_fil,how='inner',left_on=['player_id','season','week'],right_on=['player_id','season','week'])\n",
        "\n",
        "  bb_df = bb_df.drop_duplicates()\n",
        "\n",
        "  bb_df = bb_df.set_index(keys=index_cols)\n",
        "\n",
        "  #lets create the buy-sell df with defense features aggregated from future weeks (ie average of the average future opponents stats)\n",
        "\n",
        "  #the offense stats will be past looking averages, while defensive stats will be forward opponent looking\n",
        "\n",
        "  bs_df = pd.DataFrame()\n",
        "  for s in seasons:\n",
        "    season_df = full_df[full_df['season'] == s].copy()\n",
        "    min_week = season_df['week'].min()\n",
        "    max_week = season_df['week'].max()\n",
        "    for w in np.arange(min_week, max_week+1, 1):\n",
        "      current_df = season_df[season_df['week'] >= w].copy()\n",
        "      current_df_trim = current_df[index_cols+def_avg_cols].copy()\n",
        "      grouped_df = current_df_trim.groupby(index_cols)[def_avg_cols].mean().reset_index()\n",
        "\n",
        "      #indicate that the cols are no longer past looking averages for a single team. they are averages for all future season opponents\n",
        "      grouped_df.columns = index_cols + ['rem_'+c for c in def_avg_cols]\n",
        "\n",
        "      bs_df = pd.concat([bs_df,grouped_df], join='outer',axis=0)\n",
        "    \n",
        "  bs_df = pd.merge(bs_df,bs_labels_fil,how='inner',left_on=['player_id','season','week'],right_on=['player_id','season','week'])\n",
        "\n",
        "  bs_df = bs_df.drop_duplicates()\n",
        "\n",
        "  bs_df = bs_df.set_index(keys=index_cols)\n",
        "\n",
        "  return bb_df, bs_df\n",
        "  "
      ],
      "metadata": {
        "id": "rFWv_n_kZtLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQ8h3HdKNHqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_bb_bs_labels.shape"
      ],
      "metadata": {
        "id": "cRtNSda_yyQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_bb_bs_labels.sample(10)"
      ],
      "metadata": {
        "id": "bBD4xCzUDMyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bs_df.head()"
      ],
      "metadata": {
        "id": "DlfrIVpcg54x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list(all_current_week_df_veteran.columns)"
      ],
      "metadata": {
        "id": "H6pnRbtlg7d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dc9zZR7y0Ms5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Modeling Setup Pipeline"
      ],
      "metadata": {
        "id": "MLbanK-5g8Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Create Veteran and Rookie DFs containing only current week features and outcome variables (fantasy scores for each score setting)\n",
        "\n",
        "all_current_week_df_veteran, all_current_week_df_rookie = create_all_current_week_df(sched_dep_ros_inj,weekly_mas_df,defense_game_df,combine_data,\n",
        "                                                                                     seasons)\n",
        "\n",
        "#Step 2: Create dictionary with clean feature DFs by vet/rookie by position, filtered by depth chart positions, with cols relevant to the position\n",
        "\n",
        "all_current_dfs_dict = clean_and_split_by_pos_for_model(all_current_week_df_veteran, all_current_week_df_rookie, reg_pos_col_dict=reg_pos_col_dict,\n",
        "                                     pos_depth_chart_dict=pos_depth_chart_dict)\n",
        "\n",
        "#Step 3: Return dataframes that label booms-busts and buy-sells\n",
        "#booms and busts are labeled on the number of standard deviations between their avg point average during prior season weeks and actual performance that week\n",
        "#buy-sells are labeled using the difference of their mean weekly fantasy position rankings and their avg position ranking for the rest of the season\n",
        "\n",
        "final_bb_bs_labels = create_boom_bust_buy_sell_dfs(all_current_week_df_veteran, all_current_week_df_rookie, reg_pos_col_dict=reg_pos_col_dict,\n",
        "                                     pos_depth_chart_dict=pos_depth_chart_dict, boom_thres=7, bust_thres=-0.01, pos_rank_change_buy=-20,\n",
        "                                     pos_rank_change_sell=20,seasons=seasons)\n",
        "\n",
        "#Step 4: Create Clean Labeled Feature DFs for Multilayer Perceptron Classifiers for boom-bust and buy-sell\n",
        "bb_df, bs_df  = create_clean_boom_bust_buy_sell_df(all_current_week_df_veteran, all_current_week_df_rookie,final_bb_bs_labels)\n"
      ],
      "metadata": {
        "id": "JrZQ0A6ag8uT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6811e594-b6f6-42ee-996c-7a454c35d76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating feature dfs for the following weeks: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 3\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|         | 1/15 [00:03<00:52,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 4\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|        | 2/15 [00:07<00:51,  3.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 5\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 3/15 [00:11<00:44,  3.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 6\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|       | 4/15 [00:14<00:39,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 7\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|      | 5/15 [00:19<00:39,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 8\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|      | 6/15 [00:23<00:36,  4.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 9\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|     | 7/15 [00:26<00:30,  3.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 10\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|    | 8/15 [00:30<00:27,  3.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 11\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|    | 9/15 [00:34<00:22,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 12\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|   | 10/15 [00:38<00:18,  3.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 13\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|  | 11/15 [00:41<00:14,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 14\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|  | 12/15 [00:46<00:11,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 15\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%| | 13/15 [00:49<00:07,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 16\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|| 14/15 [00:53<00:03,  3.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2020 and Current Week: 17\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2020 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2020 contain 1040 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 15/15 [00:57<00:00,  3.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating feature dfs for the following weeks: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 3\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|         | 1/16 [00:02<00:44,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 4\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|        | 2/16 [00:06<00:43,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 5\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|        | 3/16 [00:09<00:40,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 6\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|       | 4/16 [00:13<00:42,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 7\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|      | 5/16 [00:16<00:36,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 8\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|      | 6/16 [00:19<00:32,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 9\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|     | 7/16 [00:24<00:34,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 10\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 8/16 [00:27<00:28,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 11\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|    | 9/16 [00:30<00:24,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 12\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|   | 10/16 [00:36<00:24,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 13\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|   | 11/16 [00:39<00:19,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 14\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|  | 12/16 [00:43<00:15,  3.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 15\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%| | 13/16 [00:46<00:11,  3.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 16\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%| | 14/16 [00:52<00:08,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 17\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|| 15/16 [00:59<00:05,  5.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2021 and Current Week: 18\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2021 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2021 contain 1124 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 16/16 [01:03<00:00,  3.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating feature dfs for the following weeks: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 3\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|         | 1/16 [00:02<00:44,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 4\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|        | 2/16 [00:05<00:41,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 5\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|        | 3/16 [00:09<00:40,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 6\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|       | 4/16 [00:13<00:41,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 7\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|      | 5/16 [00:16<00:36,  3.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 8\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|      | 6/16 [00:19<00:32,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 9\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|     | 7/16 [00:22<00:27,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 10\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 8/16 [00:26<00:26,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 11\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|    | 9/16 [00:28<00:22,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 12\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|   | 10/16 [00:36<00:26,  4.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 13\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|   | 11/16 [00:43<00:26,  5.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 14\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|  | 12/16 [00:46<00:18,  4.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 15\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%| | 13/16 [00:51<00:14,  4.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 16\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%| | 14/16 [00:55<00:08,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 17\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|| 15/16 [00:58<00:03,  3.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now aggregating player stats up to Current Season: 2022 and Current Week: 18\n",
            "Now cleaning data into relevant data points for modeling avoiding leakage, split veterans and rookies\n",
            "Attempting to load 2022 Tweets from local session CSV\n",
            "Now cleaning tweet data and looking for rookie references\n",
            "Tweets from 2022 contain 1112 tweets with fantasy rookie references\n",
            "Now assessing sentiment of rookie tweet references\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 16/16 [01:01<00:00,  3.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgMCt4QEJNzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#buy_sell.sample(10)"
      ],
      "metadata": {
        "id": "tXtFbEBzShYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##used for tuning boom/bust z score thresholds\n",
        "\n",
        "#print(vet_boom_bust.groupby(['fantasy_points_halfppr-boom','week'])['player_id'].nunique())\n",
        "#print(vet_boom_bust.groupby(['fantasy_points_halfppr-bust','week'])['player_id'].nunique())\n",
        "#print(rook_boom_bust.groupby(['fantasy_points_halfppr-boom','week'])['player_id'].nunique())\n",
        "#print(rook_boom_bust.groupby(['fantasy_points_halfppr-bust','week'])['player_id'].nunique())"
      ],
      "metadata": {
        "id": "aKXf44rdRr-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#used for tuning buy/sell thresholds\n",
        "\n",
        "#print(buy_sell.groupby(['buy','week'])['player_id'].nunique())\n",
        "#print(buy_sell.groupby(['sell','week'])['player_id'].nunique())"
      ],
      "metadata": {
        "id": "stKrcL1bQF0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #example of how to access positional dfs\n",
        "all_current_dfs_dict['vet']['RB'].head()"
      ],
      "metadata": {
        "id": "KODwQXeW9lmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "0024e644-73ab-424d-f72d-13c040224d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                            depth_team  \\\n",
              "season week player_id  player_name           report_status               \n",
              "2020   3    00-0032780 Jordan Howard         NaN                     1   \n",
              "            00-0033308 Matt Breida           NaN                     2   \n",
              "            00-0030404 Chris Thompson        NaN                     2   \n",
              "            00-0030578 Cordarrelle Patterson NaN                     1   \n",
              "            00-0033556 Tarik Cohen           NaN                     1   \n",
              "\n",
              "                                                            div_game  \\\n",
              "season week player_id  player_name           report_status             \n",
              "2020   3    00-0032780 Jordan Howard         NaN                   0   \n",
              "            00-0033308 Matt Breida           NaN                   0   \n",
              "            00-0030404 Chris Thompson        NaN                   0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                   0   \n",
              "            00-0033556 Tarik Cohen           NaN                   0   \n",
              "\n",
              "                                                            years_exp  \\\n",
              "season week player_id  player_name           report_status              \n",
              "2020   3    00-0032780 Jordan Howard         NaN                  4.0   \n",
              "            00-0033308 Matt Breida           NaN                  3.0   \n",
              "            00-0030404 Chris Thompson        NaN                  7.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                  7.0   \n",
              "            00-0033556 Tarik Cohen           NaN                  3.0   \n",
              "\n",
              "                                                            game_played_sum  \\\n",
              "season week player_id  player_name           report_status                    \n",
              "2020   3    00-0032780 Jordan Howard         NaN                        2.0   \n",
              "            00-0033308 Matt Breida           NaN                        2.0   \n",
              "            00-0030404 Chris Thompson        NaN                        2.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                        2.0   \n",
              "            00-0033556 Tarik Cohen           NaN                        2.0   \n",
              "\n",
              "                                                            carries_avg  \\\n",
              "season week player_id  player_name           report_status                \n",
              "2020   3    00-0032780 Jordan Howard         NaN                    6.5   \n",
              "            00-0033308 Matt Breida           NaN                    6.0   \n",
              "            00-0030404 Chris Thompson        NaN                    1.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                    5.5   \n",
              "            00-0033556 Tarik Cohen           NaN                    6.0   \n",
              "\n",
              "                                                            rushing_yards_avg  \\\n",
              "season week player_id  player_name           report_status                      \n",
              "2020   3    00-0032780 Jordan Howard         NaN                          5.5   \n",
              "            00-0033308 Matt Breida           NaN                         29.5   \n",
              "            00-0030404 Chris Thompson        NaN                          3.5   \n",
              "            00-0030578 Cordarrelle Patterson NaN                         22.0   \n",
              "            00-0033556 Tarik Cohen           NaN                         26.5   \n",
              "\n",
              "                                                            rushing_tds_avg  \\\n",
              "season week player_id  player_name           report_status                    \n",
              "2020   3    00-0032780 Jordan Howard         NaN                        1.0   \n",
              "            00-0033308 Matt Breida           NaN                        0.0   \n",
              "            00-0030404 Chris Thompson        NaN                        0.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                        0.0   \n",
              "            00-0033556 Tarik Cohen           NaN                        0.0   \n",
              "\n",
              "                                                            rushing_fumbles_avg  \\\n",
              "season week player_id  player_name           report_status                        \n",
              "2020   3    00-0032780 Jordan Howard         NaN                            0.0   \n",
              "            00-0033308 Matt Breida           NaN                            0.5   \n",
              "            00-0030404 Chris Thompson        NaN                            0.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                            0.0   \n",
              "            00-0033556 Tarik Cohen           NaN                            0.0   \n",
              "\n",
              "                                                            rushing_first_downs_avg  \\\n",
              "season week player_id  player_name           report_status                            \n",
              "2020   3    00-0032780 Jordan Howard         NaN                                1.5   \n",
              "            00-0033308 Matt Breida           NaN                                1.5   \n",
              "            00-0030404 Chris Thompson        NaN                                0.5   \n",
              "            00-0030578 Cordarrelle Patterson NaN                                1.5   \n",
              "            00-0033556 Tarik Cohen           NaN                                1.0   \n",
              "\n",
              "                                                            rush_20_yds_avg  \\\n",
              "season week player_id  player_name           report_status                    \n",
              "2020   3    00-0032780 Jordan Howard         NaN                        0.0   \n",
              "            00-0033308 Matt Breida           NaN                        0.0   \n",
              "            00-0030404 Chris Thompson        NaN                        0.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                        0.0   \n",
              "            00-0033556 Tarik Cohen           NaN                        0.0   \n",
              "\n",
              "                                                            ...  \\\n",
              "season week player_id  player_name           report_status  ...   \n",
              "2020   3    00-0032780 Jordan Howard         NaN            ...   \n",
              "            00-0033308 Matt Breida           NaN            ...   \n",
              "            00-0030404 Chris Thompson        NaN            ...   \n",
              "            00-0030578 Cordarrelle Patterson NaN            ...   \n",
              "            00-0033556 Tarik Cohen           NaN            ...   \n",
              "\n",
              "                                                            team_rush_att_d  \\\n",
              "season week player_id  player_name           report_status                    \n",
              "2020   3    00-0032780 Jordan Howard         NaN                       27.0   \n",
              "            00-0033308 Matt Breida           NaN                       27.0   \n",
              "            00-0030404 Chris Thompson        NaN                       31.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                       25.5   \n",
              "            00-0033556 Tarik Cohen           NaN                       25.5   \n",
              "\n",
              "                                                            pass_tds_allowed  \\\n",
              "season week player_id  player_name           report_status                     \n",
              "2020   3    00-0032780 Jordan Howard         NaN                         2.5   \n",
              "            00-0033308 Matt Breida           NaN                         2.5   \n",
              "            00-0030404 Chris Thompson        NaN                         2.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                         2.5   \n",
              "            00-0033556 Tarik Cohen           NaN                         2.5   \n",
              "\n",
              "                                                            rush_tds_allowed  \\\n",
              "season week player_id  player_name           report_status                     \n",
              "2020   3    00-0032780 Jordan Howard         NaN                         0.5   \n",
              "            00-0033308 Matt Breida           NaN                         0.5   \n",
              "            00-0030404 Chris Thompson        NaN                         1.5   \n",
              "            00-0030578 Cordarrelle Patterson NaN                         2.5   \n",
              "            00-0033556 Tarik Cohen           NaN                         2.5   \n",
              "\n",
              "                                                            passing_yards_allowed  \\\n",
              "season week player_id  player_name           report_status                          \n",
              "2020   3    00-0032780 Jordan Howard         NaN                            301.0   \n",
              "            00-0033308 Matt Breida           NaN                            301.0   \n",
              "            00-0030404 Chris Thompson        NaN                            285.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                            386.0   \n",
              "            00-0033556 Tarik Cohen           NaN                            386.0   \n",
              "\n",
              "                                                            rushing_yards_allowed  \\\n",
              "season week player_id  player_name           report_status                          \n",
              "2020   3    00-0032780 Jordan Howard         NaN                            106.5   \n",
              "            00-0033308 Matt Breida           NaN                            106.5   \n",
              "            00-0030404 Chris Thompson        NaN                            165.5   \n",
              "            00-0030578 Cordarrelle Patterson NaN                            105.0   \n",
              "            00-0033556 Tarik Cohen           NaN                            105.0   \n",
              "\n",
              "                                                            yac_allowed  \\\n",
              "season week player_id  player_name           report_status                \n",
              "2020   3    00-0032780 Jordan Howard         NaN                  150.5   \n",
              "            00-0033308 Matt Breida           NaN                  150.5   \n",
              "            00-0030404 Chris Thompson        NaN                  121.0   \n",
              "            00-0030578 Cordarrelle Patterson NaN                  182.5   \n",
              "            00-0033556 Tarik Cohen           NaN                  182.5   \n",
              "\n",
              "                                                            ypc_allowed  \\\n",
              "season week player_id  player_name           report_status                \n",
              "2020   3    00-0032780 Jordan Howard         NaN               3.953125   \n",
              "            00-0033308 Matt Breida           NaN               3.953125   \n",
              "            00-0030404 Chris Thompson        NaN               5.349013   \n",
              "            00-0030578 Cordarrelle Patterson NaN               4.189967   \n",
              "            00-0033556 Tarik Cohen           NaN               4.189967   \n",
              "\n",
              "                                                            fantasy_points  \\\n",
              "season week player_id  player_name           report_status                   \n",
              "2020   3    00-0032780 Jordan Howard         NaN                       6.1   \n",
              "            00-0033308 Matt Breida           NaN                       0.4   \n",
              "            00-0030404 Chris Thompson        NaN                       3.8   \n",
              "            00-0030578 Cordarrelle Patterson NaN                       1.3   \n",
              "            00-0033556 Tarik Cohen           NaN                       4.1   \n",
              "\n",
              "                                                            fantasy_points_halfppr  \\\n",
              "season week player_id  player_name           report_status                           \n",
              "2020   3    00-0032780 Jordan Howard         NaN                               6.1   \n",
              "            00-0033308 Matt Breida           NaN                               0.4   \n",
              "            00-0030404 Chris Thompson        NaN                               6.3   \n",
              "            00-0030578 Cordarrelle Patterson NaN                               1.3   \n",
              "            00-0033556 Tarik Cohen           NaN                               5.6   \n",
              "\n",
              "                                                            fantasy_points_ppr  \n",
              "season week player_id  player_name           report_status                      \n",
              "2020   3    00-0032780 Jordan Howard         NaN                           6.1  \n",
              "            00-0033308 Matt Breida           NaN                           0.4  \n",
              "            00-0030404 Chris Thompson        NaN                           8.8  \n",
              "            00-0030578 Cordarrelle Patterson NaN                           1.3  \n",
              "            00-0033556 Tarik Cohen           NaN                           7.1  \n",
              "\n",
              "[5 rows x 43 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f92e933-33fd-49ba-a77a-1803dc50ed17\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>depth_team</th>\n",
              "      <th>div_game</th>\n",
              "      <th>years_exp</th>\n",
              "      <th>game_played_sum</th>\n",
              "      <th>carries_avg</th>\n",
              "      <th>rushing_yards_avg</th>\n",
              "      <th>rushing_tds_avg</th>\n",
              "      <th>rushing_fumbles_avg</th>\n",
              "      <th>rushing_first_downs_avg</th>\n",
              "      <th>rush_20_yds_avg</th>\n",
              "      <th>...</th>\n",
              "      <th>team_rush_att_d</th>\n",
              "      <th>pass_tds_allowed</th>\n",
              "      <th>rush_tds_allowed</th>\n",
              "      <th>passing_yards_allowed</th>\n",
              "      <th>rushing_yards_allowed</th>\n",
              "      <th>yac_allowed</th>\n",
              "      <th>ypc_allowed</th>\n",
              "      <th>fantasy_points</th>\n",
              "      <th>fantasy_points_halfppr</th>\n",
              "      <th>fantasy_points_ppr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>season</th>\n",
              "      <th>week</th>\n",
              "      <th>player_id</th>\n",
              "      <th>player_name</th>\n",
              "      <th>report_status</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2020</th>\n",
              "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
              "      <th>00-0032780</th>\n",
              "      <th>Jordan Howard</th>\n",
              "      <th>NaN</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>301.0</td>\n",
              "      <td>106.5</td>\n",
              "      <td>150.5</td>\n",
              "      <td>3.953125</td>\n",
              "      <td>6.1</td>\n",
              "      <td>6.1</td>\n",
              "      <td>6.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00-0033308</th>\n",
              "      <th>Matt Breida</th>\n",
              "      <th>NaN</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>29.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>301.0</td>\n",
              "      <td>106.5</td>\n",
              "      <td>150.5</td>\n",
              "      <td>3.953125</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00-0030404</th>\n",
              "      <th>Chris Thompson</th>\n",
              "      <th>NaN</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>285.0</td>\n",
              "      <td>165.5</td>\n",
              "      <td>121.0</td>\n",
              "      <td>5.349013</td>\n",
              "      <td>3.8</td>\n",
              "      <td>6.3</td>\n",
              "      <td>8.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00-0030578</th>\n",
              "      <th>Cordarrelle Patterson</th>\n",
              "      <th>NaN</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>25.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>386.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>182.5</td>\n",
              "      <td>4.189967</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00-0033556</th>\n",
              "      <th>Tarik Cohen</th>\n",
              "      <th>NaN</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>26.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>25.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>386.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>182.5</td>\n",
              "      <td>4.189967</td>\n",
              "      <td>4.1</td>\n",
              "      <td>5.6</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  43 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f92e933-33fd-49ba-a77a-1803dc50ed17')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f92e933-33fd-49ba-a77a-1803dc50ed17 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f92e933-33fd-49ba-a77a-1803dc50ed17');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bb_df.head()"
      ],
      "metadata": {
        "id": "6wChFzBqZivv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seasons[:-1]"
      ],
      "metadata": {
        "id": "xGEVGOiMVYMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Step 1 - Function to train and return preds for MLP classifiers for boom-bust and buy_sell\n",
        "Shoutout to Michael Fuchs of whom we used MLP code as listed at the following link https://michael-fuchs-python.netlify.app/2021/02/03/nn-multi-layer-perceptron-classifier-mlpclassifier/\n",
        "\n",
        "Please note, this is modeling in somewhat of infancy that we added as an additional value add. Future tuning very much planned as the models very rarely identify scenarios. May require label tuning as well. Great learning experience here trying something novel :)"
      ],
      "metadata": {
        "id": "Goh-ZFRbVYtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_preds_classifier_bb(bb_df, seasons=seasons):\n",
        "\n",
        "  index_cols = ['player_id','player_name','season','week']\n",
        "  #let's train on 2020-2021 for both classifiers, test on 2022. will return 2022 preds for dashboarding\n",
        "  # first for filtering we need to reset the indexes\n",
        "  bb_df_mod = bb_df.reset_index()\n",
        "  \n",
        "  bb_df_mod = bb_df_mod.fillna(0)\n",
        "  \n",
        "\n",
        "  bb_train_df = bb_df_mod[bb_df_mod['season'].isin(seasons[:-1])].copy()\n",
        "  bb_test_df = bb_df_mod[bb_df_mod['season'] == seasons[-1]].copy()\n",
        "\n",
        "\n",
        "  #let's set indexes again\n",
        "\n",
        "  bb_train_df = bb_train_df.set_index(keys=index_cols)\n",
        "  bb_test_df = bb_test_df.set_index(keys=index_cols)\n",
        "\n",
        "\n",
        "  #lets split into train and test for each of boom bust and buy sell\n",
        "\n",
        "  bb_x_train = bb_train_df.loc[:, bb_train_df.columns != 'boom_or_bust']\n",
        "  bb_y_train = bb_train_df['boom_or_bust']\n",
        "  bb_x_test = bb_test_df.loc[:, bb_test_df.columns != 'boom_or_bust']\n",
        "  bb_y_test = bb_test_df['boom_or_bust']\n",
        "\n",
        "\n",
        "  #lets scale the data for bb\n",
        "\n",
        "  sc=StandardScaler()\n",
        "\n",
        "  scaler_bb = sc.fit(bb_x_train)\n",
        "  bb_x_train_scaled = scaler_bb.transform(bb_x_train)\n",
        "  bb_x_test_scaled = scaler_bb.transform(bb_x_test)\n",
        "\n",
        "  #let's create the MLF model for bb\n",
        "  mlp_clf_bb = MLPClassifier(hidden_layer_sizes=(5,2),\n",
        "                        max_iter = 200,activation = 'relu',\n",
        "                        solver = 'adam')\n",
        "  \n",
        "  mlp_clf_bb.fit(bb_x_train_scaled, bb_y_train)\n",
        "\n",
        "  y_pred_bb = mlp_clf_bb.predict(bb_x_test_scaled)\n",
        "\n",
        "  print('Accuracy for Boom Bust: {:.2f}'.format(accuracy_score(bb_y_test, y_pred_bb)))\n",
        "\n",
        "  pred_df = bb_y_test.reset_index()\n",
        "\n",
        "  pred_df = pred_df.drop(labels='boom_or_bust', axis=1)\n",
        "\n",
        "  pred_df['boom_or_bust'] = y_pred_bb\n",
        "\n",
        "  return pred_df\n",
        "\n",
        "#just a copy of the above for bs\n",
        "def train_and_preds_classifier_bs(bs_df, seasons=seasons):\n",
        "\n",
        "  index_cols = ['player_id','player_name','season','week']\n",
        "\n",
        "  bs_df_mod = bs_df.reset_index()\n",
        "  bs_df_mod = bs_df_mod.fillna(0)\n",
        "  bs_train_df = bs_df_mod[bs_df_mod['season'].isin(seasons[:-1])].copy()\n",
        "  bs_test_df = bs_df_mod[bs_df_mod['season'] == seasons[-1]].copy()\n",
        "  bs_train_df = bs_train_df.set_index(keys=index_cols)\n",
        "  bs_test_df = bs_test_df.set_index(keys=index_cols)\n",
        "\n",
        "  bs_x_train = bs_train_df.loc[:, bs_train_df.columns != 'buy_or_sell']\n",
        "  #print(bs_x_train.columns)\n",
        "  bs_y_train = bs_train_df['buy_or_sell']\n",
        "  bs_x_test = bs_test_df.loc[:, bs_test_df.columns != 'buy_or_sell']\n",
        "  bs_y_test = bs_test_df['buy_or_sell']\n",
        "\n",
        "  #lets scale the data for bs\n",
        "\n",
        "  sc=StandardScaler()\n",
        "\n",
        "  scaler_bs = sc.fit(bs_x_train)\n",
        "  bs_x_train_scaled = scaler_bs.transform(bs_x_train)\n",
        "  bs_x_test_scaled = scaler_bs.transform(bs_x_test)\n",
        "\n",
        "  #let's create the MLF model for bs\n",
        "  mlp_clf_bs = MLPClassifier(hidden_layer_sizes=(5,2),\n",
        "                        max_iter = 200,activation = 'relu',\n",
        "                        solver = 'adam')\n",
        "  \n",
        "  mlp_clf_bs.fit(bs_x_train_scaled, bs_y_train)\n",
        "\n",
        "  y_pred_bs = mlp_clf_bs.predict(bs_x_test_scaled)\n",
        "\n",
        "  print('Initial Accuracy for Buy Sell: {:.2f}'.format(accuracy_score(bs_y_test, y_pred_bs)))\n",
        "\n",
        "  #low accuracy so let's tune params\n",
        "\n",
        "  #param_grid = {\n",
        "  #  'hidden_layer_sizes': [(10,8,2), (12,10,5), (14,12,6)],\n",
        "  #  'max_iter': [50, 100, 150],\n",
        "  #  'activation': ['tanh', 'relu'],\n",
        "  #  'solver': ['sgd', 'adam'],\n",
        "  #  'alpha': [0.0001, 0.05],\n",
        "  #  'learning_rate': ['constant','adaptive'],\n",
        "  #}\n",
        "\n",
        "  #grid = GridSearchCV(mlp_clf_bs, param_grid, n_jobs= -1, cv=5)\n",
        "  #grid.fit(bs_x_train_scaled, bs_y_train)\n",
        "\n",
        "  #print(grid.best_params_) \n",
        "\n",
        "  #grid_predictions = grid.predict(bs_x_test_scaled) \n",
        "\n",
        "  #print('Initial Accuracy for Buy Sell: {:.2f}'.format(accuracy_score(bs_y_test, grid_predictions)))\n",
        "\n",
        "  pred_df = bs_y_test.reset_index()\n",
        "\n",
        "  pred_df = pred_df.drop(labels='buy_or_sell', axis=1)\n",
        "\n",
        "  pred_df['buy_or_sell'] = y_pred_bs\n",
        "\n",
        "  return pred_df"
      ],
      "metadata": {
        "id": "n5kCNXrhVh8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zA6qQxPbc387"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jq6UbIkikfUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc5tUBX-pazD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Step 2 - Function to Run Recursive Feature Elimination and Identify Optimal Regressions for each experience, position combination by scoring format"
      ],
      "metadata": {
        "id": "zmmH6oqGl6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set variables needed in the regression runs\n",
        "clean_pos_results_dict = all_current_dfs_dict\n",
        "regression_list = ['Linear','Ridge','Lasso','ElasticNet','DecisionTree']\n",
        "models = {'Linear':LinearRegression(), 'Ridge': Ridge(), 'Lasso': Lasso(), 'ElasticNet': ElasticNet(), 'DecisionTree': DecisionTreeRegressor() }\n",
        "scoring_formats = ['fantasy_points','fantasy_points_halfppr','fantasy_points_ppr']\n",
        "positions = ['QB', 'RB', 'WR', 'TE']\n",
        "experience = ['vet','rookie']\n",
        "test_size = 0.2\n",
        "random_state = 28"
      ],
      "metadata": {
        "id": "zJcCY54xd-HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STEP1\n",
        "##Run Recursive Feature Elimination and model for each combination of experience, position, and scoring format (24 combinations) \n",
        "##Run through each model type\n",
        "##For each combination of experience, position, and scoring format. output the model name with the best (closest to zero) negative mean absolute error\n",
        "def do_model(df, sformat, scoring_formats, model):\n",
        "  y = df[sformat]\n",
        "  drop_df = df.drop(labels=scoring_formats, axis=1) #drop all outcome variables\n",
        "  X = drop_df\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "  rfe = RFE(estimator=model, n_features_to_select=10)\n",
        "  pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "  # evaluate model\n",
        "  cv = RepeatedKFold(n_splits=2, n_repeats=3, random_state=1)\n",
        "  n_scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "  return mean(n_scores)\n",
        "\n",
        "##dict with all identified optimal regressions for each combination of experience, position, and scoring format\n",
        "regressions_mae = {}\n",
        "for exp,d in clean_pos_results_dict.items():\n",
        "  for sformat in scoring_formats:\n",
        "    for position,df in d.items():\n",
        "      \n",
        "      # select proper df\n",
        "      df = clean_pos_results_dict[exp][position]\n",
        "      key = f'{exp}-{sformat}-{position}'\n",
        "      results = {}\n",
        "      for regression in regression_list:\n",
        "        results[regression] = do_model(df,sformat,scoring_formats,models[regression])\n",
        "      \n",
        "      best_score = 0\n",
        "      best_key = None\n",
        "      for k,v in results.items():\n",
        "        if best_key == None:\n",
        "          best_score = v\n",
        "          best_key = k\n",
        "        else:\n",
        "          if  v > best_score:\n",
        "            best_key = k\n",
        "            best_score = v\n",
        "\n",
        "\n",
        "      regressions_mae[key] = (best_key, best_score)\n"
      ],
      "metadata": {
        "id": "uzv9HQKJcQa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Output from the code block above -- this is a dictionary that contains each experience, scoring, position format followed by \n",
        "# a tuple that shows the optimal model and the negative MAE associated with that model\n",
        "print(regressions_mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW1cuMh_ravM",
        "outputId": "524a68d0-4711-4114-b299-ab32ee8f503c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vet-fantasy_points-QB': ('ElasticNet', -6.179444309255026), 'vet-fantasy_points-RB': ('ElasticNet', -4.902816606031439), 'vet-fantasy_points-WR': ('Ridge', -4.082270086538817), 'vet-fantasy_points-TE': ('Lasso', -3.5933153314941055), 'vet-fantasy_points_halfppr-QB': ('ElasticNet', -6.182736248111719), 'vet-fantasy_points_halfppr-RB': ('Lasso', -5.228690137944158), 'vet-fantasy_points_halfppr-WR': ('Ridge', -4.72177541811502), 'vet-fantasy_points_halfppr-TE': ('Lasso', -4.135575071399704), 'vet-fantasy_points_ppr-QB': ('ElasticNet', -6.186766162250624), 'vet-fantasy_points_ppr-RB': ('Lasso', -5.626067381942966), 'vet-fantasy_points_ppr-WR': ('Ridge', -5.412709675183467), 'vet-fantasy_points_ppr-TE': ('Ridge', -4.734878973812003), 'rookie-fantasy_points-QB': ('ElasticNet', -6.179444309255026), 'rookie-fantasy_points-RB': ('ElasticNet', -4.902816606031439), 'rookie-fantasy_points-WR': ('Ridge', -4.082270086538817), 'rookie-fantasy_points-TE': ('Lasso', -3.5933153314941055), 'rookie-fantasy_points_halfppr-QB': ('ElasticNet', -6.182736248111719), 'rookie-fantasy_points_halfppr-RB': ('Lasso', -5.228690137944158), 'rookie-fantasy_points_halfppr-WR': ('Ridge', -4.72177541811502), 'rookie-fantasy_points_halfppr-TE': ('Lasso', -4.135575071399704), 'rookie-fantasy_points_ppr-QB': ('ElasticNet', -6.186766162250624), 'rookie-fantasy_points_ppr-RB': ('Lasso', -5.626067381942966), 'rookie-fantasy_points_ppr-WR': ('Ridge', -5.412709675183467), 'rookie-fantasy_points_ppr-TE': ('Ridge', -4.734878973812003)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Step 3 - Function to generate predictions for each optimal regression for each experience, position by scoring format"
      ],
      "metadata": {
        "id": "OQdwiDLRmspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Variable split lets us know whether we want to train test split or train on the full df and predict using the current week df\n",
        "#This will be relevant when we productionize into a weekly approach for future seasons\n",
        "\n",
        "def do_fit_new(df, sformat, scoring_formats, model, split=True, current_df=None):\n",
        "  # print(df.shape)\n",
        "  if split == True:\n",
        "    y = df[sformat]\n",
        "    drop_df = df.drop(labels=scoring_formats, axis=1) #drop all outcome variables\n",
        "    X = drop_df\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    rfe = RFE(estimator=model, n_features_to_select=10)\n",
        "    pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "    # make predictions\n",
        "    reg = model.fit(X_train, y_train)\n",
        "    preds = reg.predict(X_test)\n",
        "    return preds\n",
        "  elif split == False:\n",
        "    y_train = df[sformat]\n",
        "    drop_df = df.drop(labels=scoring_formats, axis=1) #drop all outcome variables\n",
        "    X_train = drop_df\n",
        "    y_test = current_df[sformat]\n",
        "    drop_df_current = current_df.drop(labels=scoring_formats, axis=1) #drop all outcome variables\n",
        "    X_test = drop_df_current\n",
        "    rfe = RFE(estimator=model, n_features_to_select=10)\n",
        "    pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "    # make predictions\n",
        "    reg = model.fit(X_train, y_train)\n",
        "    preds = reg.predict(X_test)\n",
        "    return preds\n",
        "\n",
        "predictions = {}\n",
        "for r,t in regressions_mae.items():\n",
        "  parts = r.split('-')\n",
        "  exp = parts[0]\n",
        "  position = parts[2]\n",
        "  df = clean_pos_results_dict[exp][position]\n",
        "  selected_model = t[0]\n",
        "  predictions[r] = do_fit_new(df, sformat, scoring_formats, models[selected_model])"
      ],
      "metadata": {
        "id": "2AYbaXalnfYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Step 4 - Use optimal regression function from Modeling Step 1 and predictions function from Step 3 to train iteratively up through current week in current season"
      ],
      "metadata": {
        "id": "KjyFG4jUsFro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BpVToUHLvx5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this approach limits data leakage as each week is trained only using feature data up through that week\n",
        "# pred_weeks is a list of weeks in the current season for which to generate score predictions\n",
        "\n",
        "def curr_season_curr_week_preds(all_current_dfs_dict, pred_weeks, ypreds_bb, ypreds_bs, regressions_mae=regressions_mae,\n",
        "                                current_season=current_season, week_cutoff=week_cutoff,\n",
        "                                positions=positions, scoring_formats=scoring_formats, experience=experience, models=models, team_df=team_df):\n",
        "\n",
        "  #ensure no values are provided before cutoff week in current season\n",
        "  pred_weeks = [w for w in pred_weeks if w >= week_cutoff]\n",
        "  first_pred_week = min(pred_weeks)\n",
        "  last_pred_week = max(pred_weeks)\n",
        "\n",
        "  #try to read in previous iteratively trained predictions for current_season\n",
        "  try:\n",
        "    print('Attempting to read in prior predictions for {} Season, Weeks {}-{}'.format(str(current_season),str(first_pred_week),str(last_pred_week)))\n",
        "    current_season_preds = pd.read_csv('current_week_preds_{}_season_weeks_{}_{}.csv'.format(str(current_season),str(first_pred_week),str(last_pred_week)))\n",
        "    return current_season_preds\n",
        "  except:\n",
        "    print('No matching CSV Found. Now executing training and predictions for each defined prediction week for each experience, position, scoring format combination')\n",
        "    predictions_df = pd.DataFrame()\n",
        "    for s in tqdm(scoring_formats):\n",
        "      scoring_formats_df = pd.DataFrame()\n",
        "      for w in pred_weeks:\n",
        "        #week_pred_df = pd.DataFrame()\n",
        "        for e in experience:\n",
        "          for p in positions:\n",
        "          \n",
        "            #set key to lookup optimal regression in regressions_mae\n",
        "            key = f'{e}-{s}-{p}'\n",
        "            #need to reset_index to split df on week as df will contain all seasons rows at this point\n",
        "            df = all_current_dfs_dict[e][p].reset_index()\n",
        "            #split out training df with every record in prior seasons as well as through pred week (w)\n",
        "            train_df = df[(df['season'] < current_season) | ((df['season'] == current_season) & (df['week'] < w))].copy()\n",
        "            #split out test df with records in current season and week\n",
        "            test_df = df[(df['season'] == current_season) & (df['week'] == w)].copy()\n",
        "\n",
        "            #set indexes back\n",
        "            train_df = train_df.set_index(keys=['season','week','player_id','player_name','report_status'])\n",
        "            test_df = test_df.set_index(keys=['season','week','player_id','player_name','report_status'])\n",
        "            \n",
        "            #print('Week: {} Exp: {} Pos: {} Training DF has {} rows'.format(str(w),e,p,str(train_df.shape[0])))\n",
        "            #print('Week: {} Exp: {} Pos: {} Test DF has {} rows'.format(str(w),e,p,str(test_df.shape[0])))\n",
        "\n",
        "            opt_model = regressions_mae[key][0]\n",
        "            preds = do_fit_new(train_df, s, scoring_formats, models[opt_model], split=False, current_df=test_df)\n",
        "\n",
        "            test_df = test_df.reset_index()\n",
        "            results_df = test_df[['season','week','player_id','player_name','report_status']].copy()\n",
        "\n",
        "            #add preds as column titled with the scoring format\n",
        "    \n",
        "            results_df[s] = [round(p,2) for p in preds]\n",
        "            results_df['position'] = p\n",
        "\n",
        "            #need to override any players with an Out or Doubtful injury report status as 0, or negative predictions as 0\n",
        "            results_df[s] = np.where(results_df[s] >= 0, results_df[s], 0)\n",
        "            #results_df[s] = np.where(results_df['report_status'].isin(['Out','Doubtful']), 0, results_df[s])\n",
        "\n",
        "            #veritcally concat all results to the scoring format level\n",
        "            scoring_formats_df = pd.concat([scoring_formats_df,results_df],join='outer',axis=0)\n",
        "\n",
        "      #set index\n",
        "      scoring_formats_df = scoring_formats_df.set_index(keys=['season','week','player_id','player_name','report_status','position'])\n",
        "      #horizontally concat all scoring_formats_df into the final preds_df to add each scoring column\n",
        "      predictions_df = pd.concat([predictions_df,scoring_formats_df],join='outer',axis=1,ignore_index=False)\n",
        "      #predictions_df = pd.concat([predictions_df,results_df],join='outer',axis=1,ignore_index=False)\n",
        "\n",
        "    predictions_df = predictions_df.drop_duplicates()\n",
        "\n",
        "    ###to comment out, just for dashboard testing###\n",
        "    #predictions_df['boom_or_bust'] = random.choices(['boom','bust',np.nan], k=len(predictions_df),weights=[.1,.1,.8])\n",
        "    #predictions_df['buy_or_sell'] = random.choices(['buy','sell',np.nan], k=len(predictions_df),weights=[.1,.1,.8])\n",
        "    ###end comment out###\n",
        "\n",
        "    predictions_df = predictions_df.reset_index()\n",
        "\n",
        "    #merge with team dfs to get player teams\n",
        "\n",
        "    predictions_df = pd.merge(predictions_df,team_df,how='inner',left_on=['player_id','season','week'], right_on=['player_id','season','week'])\n",
        "\n",
        "    #let's merge with our boom or bust predictions\n",
        "    predictions_df = pd.merge(predictions_df,ypreds_bb,how='inner',left_on=['player_id','player_name','season','week'], right_on=['player_id','player_name','season','week'])\n",
        "    #print(predictions_df.columns)\n",
        "    predictions_df = pd.merge(predictions_df,ypreds_bs,how='inner',left_on=['player_id','player_name','season','week'], right_on=['player_id','player_name','season','week'])\n",
        "   # print(predictions_df.columns)\n",
        "    #print(predictions_df.columns)\n",
        "\n",
        "    predictions_df = predictions_df.drop_duplicates()\n",
        "\n",
        "    #keep cols needed for dashboarding\n",
        "    final_predictions_df = predictions_df[['season','week','player_name','teams','position','fantasy_points','fantasy_points_halfppr','fantasy_points_ppr',\n",
        "                                           'report_status','boom_or_bust','buy_or_sell']].copy()\n",
        "    #rename cols\n",
        "    final_predictions_df.columns = ['season','week','player','team','position','standard','half_ppr','ppr',\n",
        "                                           'injury_status','boom_or_bust','buy_or_sell']\n",
        "\n",
        "    bb_norm_dict = {1:'boom',-1:'bust',0:np.nan}\n",
        "    bs_norm_dict = {1:'buy',-1:'sell',0:np.nan}\n",
        "\n",
        "    final_predictions_df['boom_or_bust'] = final_predictions_df['boom_or_bust'].map(bb_norm_dict)\n",
        "    final_predictions_df['buy_or_sell'] = final_predictions_df['buy_or_sell'].map(bs_norm_dict)\n",
        "\n",
        "    #final_predictions_df = final_predictions_df.fillna('')\n",
        "\n",
        "\n",
        "    final_predictions_df = final_predictions_df.sort_values(by=['season','week','position','standard'], ascending=[True,True,True,False])\n",
        "    #export predictions as a csv to avoid duplicating future runs\n",
        "    print('Now exporting final prediction dataframe as CSV')\n",
        "    final_predictions_df.to_csv('current_week_preds_{}_season_weeks_{}_{}.csv'.format(str(current_season),str(first_pred_week),str(last_pred_week)),header=True,index=False)\n",
        "\n",
        "    return final_predictions_df\n"
      ],
      "metadata": {
        "id": "nBa9QDfisDr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgI5wGS303C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute modeling pipeline"
      ],
      "metadata": {
        "id": "C8X8b52EWB9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate preds for boom bust and buy sell\n",
        "ypreds_bb = train_and_preds_classifier_bb(bb_df, seasons=seasons)\n",
        "ypreds_bs = train_and_preds_classifier_bs(bs_df, seasons=seasons)\n",
        "\n",
        "#Generate final consolidated preds file for dashboarding. hoorah!\n",
        "preds_df = curr_season_curr_week_preds(clean_pos_results_dict,list(np.arange(week_cutoff,19,1)),ypreds_bb,ypreds_bs)"
      ],
      "metadata": {
        "id": "fIUQOjgT33QQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480a36e6-51ce-411f-9229-ecf167664ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Boom Bust: 0.98\n",
            "Initial Accuracy for Buy Sell: 0.63\n",
            "Attempting to read in prior predictions for 2022 Season, Weeks 3-18\n",
            "No matching CSV Found. Now executing training and predictions for each defined prediction week for each experience, position, scoring format combination\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3/3 [00:19<00:00,  6.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now exporting final prediction dataframe as CSV\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df.head()"
      ],
      "metadata": {
        "id": "ZdnpqSHwsnpq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "18b187f6-7ea3-4e82-c43d-5857f3c16093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    season  week         player team position  standard  half_ppr    ppr  \\\n",
              "18    2022     3    Jalen Hurts  PHI       QB     22.80     22.83  22.86   \n",
              "12    2022     3  Lamar Jackson  BAL       QB     21.16     21.18  21.19   \n",
              "19    2022     3   Carson Wentz  WAS       QB     21.12     21.13  21.15   \n",
              "8     2022     3     Josh Allen  BUF       QB     19.93     19.93  19.94   \n",
              "16    2022     3     Derek Carr   LV       QB     19.54     19.55  19.56   \n",
              "\n",
              "   injury_status boom_or_bust buy_or_sell  \n",
              "18                                         \n",
              "12                                         \n",
              "19                                         \n",
              "8                                          \n",
              "16                                         "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c058574-4a66-45a5-b779-3a343a2ac112\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>season</th>\n",
              "      <th>week</th>\n",
              "      <th>player</th>\n",
              "      <th>team</th>\n",
              "      <th>position</th>\n",
              "      <th>standard</th>\n",
              "      <th>half_ppr</th>\n",
              "      <th>ppr</th>\n",
              "      <th>injury_status</th>\n",
              "      <th>boom_or_bust</th>\n",
              "      <th>buy_or_sell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>Jalen Hurts</td>\n",
              "      <td>PHI</td>\n",
              "      <td>QB</td>\n",
              "      <td>22.80</td>\n",
              "      <td>22.83</td>\n",
              "      <td>22.86</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>Lamar Jackson</td>\n",
              "      <td>BAL</td>\n",
              "      <td>QB</td>\n",
              "      <td>21.16</td>\n",
              "      <td>21.18</td>\n",
              "      <td>21.19</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>Carson Wentz</td>\n",
              "      <td>WAS</td>\n",
              "      <td>QB</td>\n",
              "      <td>21.12</td>\n",
              "      <td>21.13</td>\n",
              "      <td>21.15</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>Josh Allen</td>\n",
              "      <td>BUF</td>\n",
              "      <td>QB</td>\n",
              "      <td>19.93</td>\n",
              "      <td>19.93</td>\n",
              "      <td>19.94</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>Derek Carr</td>\n",
              "      <td>LV</td>\n",
              "      <td>QB</td>\n",
              "      <td>19.54</td>\n",
              "      <td>19.55</td>\n",
              "      <td>19.56</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c058574-4a66-45a5-b779-3a343a2ac112')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c058574-4a66-45a5-b779-3a343a2ac112 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c058574-4a66-45a5-b779-3a343a2ac112');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connection to Dashboard"
      ],
      "metadata": {
        "id": "qYkpg2Yu3C9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "def write_to_sheet(df, sheet_id, sheet_range):\n",
        "\n",
        "   # Create the service client\n",
        "   service = build('sheets', 'v4')\n",
        "\n",
        "   # Clear the existing values in the sheet\n",
        "   service.spreadsheets().values().clear(\n",
        "       spreadsheetId=sheet_id, range=sheet_range).execute()\n",
        "\n",
        "   # Write the DataFrame to the sheet\n",
        "   values = df.values.tolist()\n",
        "   body = {\n",
        "       'values': values\n",
        "   }\n",
        "   result = service.spreadsheets().values().update(\n",
        "       spreadsheetId=sheet_id, range=sheet_range,\n",
        "       valueInputOption='USER_ENTERED', body=body).execute()\n",
        "   print('{0} cells updated.'.format(result.get('updatedCells')))\n",
        "\n",
        "# The existing sheet we want to overwrite in Drive\n",
        "sheet_id = '1KMhP6fn_4prOEYs284vBESojTH0J0hPkdwNQRb2EuC8'\n",
        "sheet_range = 'project_data!A2:K'\n",
        "\n",
        "# Execute the overwrite with data outputted from models in df (prediction_data)\n",
        "write_to_sheet(preds_df[['season','week','player','team','position','standard',\n",
        "                                'half_ppr','ppr','injury_status','boom_or_bust',\n",
        "                                'buy_or_sell']], sheet_id, sheet_range)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWzVIG7b-5Ke",
        "outputId": "a969649e-b82d-432d-be1d-7f9a9ff3eeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32076 cells updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End Pipeline, below for illustration and eval only"
      ],
      "metadata": {
        "id": "QW6m2N7lV9AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output predictions for future investigation/eval\n",
        "\n",
        "predictions = {}\n",
        "for r,t in regressions_mae.items():\n",
        "  parts = r.split('-')\n",
        "  exp = parts[0]\n",
        "  position = parts[2]\n",
        "  df = clean_pos_results_dict[exp][position]\n",
        "  selected_model = t[0]\n",
        "  predictions[r] = do_fit_new(df, sformat, scoring_formats, models[selected_model],split=True)"
      ],
      "metadata": {
        "id": "UfC_2F67r1jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print out predictions of X_test -- this is a dictionary in the format of {experience-scoring-position: [predictions array]} for each of the 24 combinations\n",
        "print(predictions['vet-fantasy_points-QB']) #this is just printing out predictions for veteran qbs in standard scoring, but the predictions dictionary has all of them"
      ],
      "metadata": {
        "id": "1FujMFrewbOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two blocks above do all that is needed for getting the predictions on the test set"
      ],
      "metadata": {
        "id": "58Y1ZK2a411W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1\n",
        "#Create Veteran DFs\n",
        "vet_qb_df = all_current_dfs_dict['vet']['QB']\n",
        "vet_rb_df = all_current_dfs_dict['vet']['RB']\n",
        "vet_wr_df = all_current_dfs_dict['vet']['WR']\n",
        "vet_te_df = all_current_dfs_dict['vet']['TE']\n",
        "\n",
        "#Create Rookie DFs\n",
        "rook_qb_df = all_current_dfs_dict['rookie']['QB']\n",
        "rook_rb_df = all_current_dfs_dict['rookie']['RB']\n",
        "rook_wr_df = all_current_dfs_dict['rookie']['WR']\n",
        "rook_te_df = all_current_dfs_dict['rookie']['TE']"
      ],
      "metadata": {
        "id": "9HBjE1-d3bdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below boxes are just for data visualization -- I'm thinking for the blog I'll put in some of the graphs below -- I'm mainly just looking at one of the df combinations and showing some feature selection/correlation matrices etc for it"
      ],
      "metadata": {
        "id": "aGuCSIKV5LnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Create training and testing data sets, split by different scoring options\n",
        "\n",
        "#make X and y df for every vet, position, scoring combo -- should be 24 in total\n",
        "\n",
        "#Veteran QBs\n",
        "vet_qb_X = vet_qb_df.iloc[:,:-3]\n",
        "vet_qb_y_stand = vet_qb_df['fantasy_points']\n",
        "\n",
        "\n",
        "#Split into train and test sets for each of the 24 dataframes\n",
        "X_train, X_test, y_train, y_test = train_test_split(vet_qb_X, vet_qb_y_stand, test_size=0.2, random_state=28)"
      ],
      "metadata": {
        "id": "1OIN4wjlQ0Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Quick Visualizations (2 Options) Showing Feature Importance -- Use graphs to justify n selection for selectkbest features in next step\n",
        "#Additional Feature Selection Visualizations...Pick one of these two \n",
        "\n",
        "#Feature Selection For Correlation\n",
        "\n",
        "# feature selection\n",
        "f_selector = SelectKBest(score_func=f_regression, k='all')\n",
        "# learn relationship from training data\n",
        "f_selector.fit(X_train, y_train)\n",
        "# transform train input data\n",
        "X_train_fs = f_selector.transform(X_train)\n",
        "# transform test input data\n",
        "X_test_fs = f_selector.transform(X_test)\n",
        "# Plot the scores for the features\n",
        "plt.bar([i for i in range(len(f_selector.scores_))], f_selector.scores_, color = 'purple')\n",
        "plt.xlabel(\"feature index\")\n",
        "plt.ylabel(\"F-value (transformed from the correlation values)\")\n",
        "plt.title('F-value by Feature')\n",
        "plt.show()\n",
        "\n",
        "#Feature Selection for Mutual Information\n",
        "\n",
        "# feature selection\n",
        "f_selector = SelectKBest(score_func=mutual_info_regression, k='all')\n",
        "# learn relationship from training data\n",
        "f_selector.fit(X_train, y_train)\n",
        "# transform train input data\n",
        "X_train_fs = f_selector.transform(X_train)\n",
        "# transform test input data\n",
        "X_test_fs = f_selector.transform(X_test)\n",
        "# Plot the scores for the features\n",
        "plt.bar([i for i in range(len(f_selector.scores_))], f_selector.scores_, color = 'green')\n",
        "plt.xlabel(\"feature index\")\n",
        "plt.ylabel(\"Estimated MI value\")\n",
        "plt.title(\"Estimated Mutual Information by Feature\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#The y-axis represents the estimated mutual information between each feature and the target variable"
      ],
      "metadata": {
        "id": "qSN5zSdvSzKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4 Reduce Size of X_train to only select n best features -- Using selectKbest to do so\n",
        "\n",
        "# Create and fit selector\n",
        "selector = SelectKBest(f_regression, k=10)\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "# Get columns to keep and create new dataframe with those only\n",
        "cols_idxs = selector.get_support(indices=True)\n",
        "X_train_new = X_train.iloc[:,cols_idxs]"
      ],
      "metadata": {
        "id": "5BDitFlUPD6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5:\n",
        "#Pull out the k best chosen features using their column names and append the scoring type column to that list\n",
        "cols_list = X_train_new.columns.tolist()\n",
        "cols_list.append('fantasy_points')\n",
        "\n",
        "#Create df for correlation heatmap using selected features and scoring column above -- focusing on correlation to score column\n",
        "correlation_df = vet_qb_df[cols_list]\n",
        "\n",
        "#This is primarily just for visualization purposes for the blog report"
      ],
      "metadata": {
        "id": "HO5RHTF3PwJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6 Use correlation df above to get correlation heatmap for given reduced df\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n",
        "# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\n",
        "heatmap = sns.heatmap(correlation_df.corr(), vmin=-1, vmax=1, annot=True)\n",
        "# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
      ],
      "metadata": {
        "id": "F0csY3E4QLCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter Plot of Predictions Vs Actual\n",
        "\n",
        "#Example: Veteran Qbs in Standard Scoring\n",
        "\n",
        "#Veteran QBs\n",
        "vet_qb_X = vet_qb_df.iloc[:,:-3]\n",
        "vet_qb_y_stand = vet_qb_df['fantasy_points']\n",
        "\n",
        "#Split into train and test sets for each of the 24 dataframes\n",
        "X_train, X_test, y_train, y_test = train_test_split(vet_qb_X, vet_qb_y_stand, test_size=0.2, random_state=28)\n",
        "\n",
        "test_pred = predictions['vet-fantasy_points-QB']\n",
        "\n",
        "plt.scatter(y_test, test_pred)\n",
        "m, b = np.polyfit(y_test, test_pred, 1)\n",
        "#use black as color for regression line\n",
        "plt.plot(y_test, m*y_test+b, color='black')\n",
        "\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted Values -- Vet QBs Standard Scoring')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cAvvfxZY5bKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBd_fhFcq-yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpsFMqeazpoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8I1l82wwVhVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T20zwrlVVXvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-G2ObazrVYCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AiBVIjkVYdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXnO-K5bWfaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgdGWEwsWjcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vf8gp4IdW2GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwSHbpr6XEH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGf5omtbX_iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}